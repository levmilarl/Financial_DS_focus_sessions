{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Models, OLS and MLE\n",
    "\n",
    "Dear students,\n",
    "\n",
    "finance data is most often in the format of a time-series. Time-series methods for predicting the return density are therefore one key element. Be reminded that the mean (standard deviation) of that density would coincide with the expected return (return volatility).\n",
    "\n",
    "The backbone of time series analysis is the linear regression model. In this section we are repeating all about regression work that one needs for financial data science work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Online Resources from the Chair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.1 The Linear Regression Model (LRM)\n",
    "\n",
    "https://www.youtube.com/watch?v=2YHv6CeauIQ&list=PLyQSjcv8LwAHTsOSV2WUiOCRtbZNCvBrh&index=1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.2 Estimating LRMs with Ordinary Least Squares (OLS)\n",
    "\n",
    "https://www.youtube.com/watch?v=xt8F5TySAVY&list=PLyQSjcv8LwAHTsOSV2WUiOCRtbZNCvBrh&index=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.3  Maximum Likelihood: A Bird's Eye View\n",
    "\n",
    "https://www.youtube.com/watch?v=wjMtiKn38W4&list=PLyQSjcv8LwAHTsOSV2WUiOCRtbZNCvBrh&index=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.4 What is the Probability of a 10% Decline in the S\\&P 500?\n",
    "\n",
    "https://www.youtube.com/watch?v=zwubzhauDfA&list=PLyQSjcv8LwAHTsOSV2WUiOCRtbZNCvBrh&index=9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.5 Maximum Likelihood for Gaussian Linear Regression Models\n",
    "\n",
    "https://www.youtube.com/watch?v=DqKJBZUZ6iI&list=PLyQSjcv8LwAHTsOSV2WUiOCRtbZNCvBrh&index=12\n",
    "\n",
    "https://www.youtube.com/watch?v=dtbCo8QJk7U&list=PLyQSjcv8LwAHTsOSV2WUiOCRtbZNCvBrh&index=13\n",
    "\n",
    "https://www.youtube.com/watch?v=-q0D2rRNm64&list=PLyQSjcv8LwAHTsOSV2WUiOCRtbZNCvBrh&index=14\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:42:20.934671Z",
     "start_time": "2025-06-19T12:42:20.930633Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:42:20.948756Z",
     "start_time": "2025-06-19T12:42:20.945156Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B The Linear Regression Model (LRM)\n",
    "\n",
    "\n",
    "## B.1 The LRM in Compact Form\n",
    "\n",
    "\n",
    "The LRM in compact form reads:\n",
    "\n",
    "$$\n",
    "y := X\\; \\beta + \\epsilon, \\; \\epsilon :\\sim i.i.d.(0,\\sigma^2_{\\epsilon} I)\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "\\begin{align*}\n",
    "y &:= (y_1, y_2, ..., y_T)', \\text{observed}\\\\\\\\\n",
    "X &:= \\begin{pmatrix} \n",
    "\t\t\t\t\t\t\t\t\t\t\t1 & x_{11} & ... & x_{1p} \\\\\n",
    "\t\t\t\t\t\t\t\t\t\t\t... & ... & ... & ... \\\\\n",
    "\t\t\t\t\t\t\t\t\t\t\t1 & x_{T1} & ... & x_{Tp} \n",
    "\t\t  \\end{pmatrix}; \\text{observed} \\\\\\\\\n",
    "\\beta &:= [\\beta_0, \\beta_1, ..., \\beta_p]'; \\text{unknown in real-life applications} \\\\\\\\\n",
    "\\epsilon &:= (\\epsilon_1, ..., \\epsilon_T)'; \\text{unknown in real-life applications}\n",
    "\\end{align*}\n",
    " \n",
    "Let's pick one element to talk it through; here I pick $y_1$. The linear regression model says that the observed $y_1$ is a linear function in a constant and p explanatory variables $\\{x_{11}, ..., x_{1p}\\}$ plus an additive error term $\\epsilon_1$, i.e.\n",
    "\n",
    "$$\n",
    "y_1 := \\beta_0 + \\sum_{k=1}^p \\beta_k \\times x_{1k} + \\epsilon_1, \\; \\epsilon_1 \\sim iid(0,\\sigma^2_{\\epsilon}).\n",
    "$$\n",
    "\n",
    "The linear regression model assumes that the $y$ and $x$ variables are observed by the investor and that the relationship between both is indeed linear. The realized innovation terms $\\epsilon$ are only known if the regression slope coefficient vector $\\beta$ was known. Most models assume that vector to be known. Regardless of that, in real-life applications, $\\beta$ needs to be estimated. We will therefore also estimate the realized innovations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "There is one big advantage if you are able to re-write a parametric model as a linear regression model. That advantage is that there are well developed methods out there that one can use to estimate the parameters of a linear regression model. In general, every estimation procedure consists of the same steps. \n",
    "First, our objective will be to estimate the parameter vector $\\beta$ and the variance parameter of the White Noise process $\\sigma^2_{\\epsilon}$ and eventually, to also back out the sequence of realized innovations $\\epsilon$. All estimates obtain a 'hat' on top which looks like $\\hat{\\beta}$, $\\hat{\\sigma}^2_{\\epsilon}$ and $\\hat{\\epsilon}$. \n",
    "Second, we test whether $\\hat{\\epsilon}$ follows an iid distribution. \n",
    "Third, we determine the precision with which we have estimated $\\hat{\\beta}$ and $\\hat{\\sigma}^2_{\\epsilon}$. \n",
    "Fourth, and lastly, we evaluate how much of the variations in $y$ are explained by the parametric model $X\\beta$ and how much are due to unpredictable news (which is captured by $\\sigma^2_{\\epsilon}$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.2 Example: AR(1) as LRM\n",
    "\n",
    "The AR(1) model for $y$\n",
    "$$\n",
    "y_t := \\phi_0 + \\phi_1\\times y_{t-1} + \\epsilon_t; \\; \\epsilon_t \\sim iid(0,\\sigma^2_{\\epsilon}).\n",
    "$$\n",
    "\n",
    "can be rewritten as a LRM, i.e.\n",
    "\n",
    "\\begin{align*}\n",
    "y &:= X\\beta + \\epsilon \\\\\\\\\n",
    "\\underbrace{y}_{(T-1)\\times 1} &= (y_2, ..., y_T)'\\\\\\\\\n",
    "\\underbrace{X}_{(T-1) \\times 2} &= \\begin{pmatrix} 1 & y_1 \\\\ ... & ... \\\\ 1 & y_{T-1} \\end{pmatrix} \\\\\\\\\n",
    "\\underbrace{\\epsilon}_{(T-1)\\times 1} &= (\\epsilon_2, ..., \\epsilon_{T})' \\, :\\sim iid (0,\\sigma^2_{\\epsilon})\\\\\\\\\n",
    "\\beta &= (\\phi_0, \\phi_1)'.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.3 Example: ARMA(1,1) as LRM\n",
    "\n",
    "ARMA(1,1) for $y$\n",
    "\n",
    "$$\n",
    "y_t := \\phi_0 + \\phi_1 y_{t-1} + \\theta_1 \\epsilon_{t-1} + \\epsilon_t, \\; \\epsilon_t :\\sim iid(0,\\sigma^2_{\\epsilon})\n",
    "$$\n",
    "\n",
    "can be rewritten as a LRM, i.e.\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "\\begin{align*}\n",
    "y = X\\beta + \\epsilon \\\\\\\\\n",
    "\\underbrace{y}_{(T-1)\\times 1} &= (y_2, ..., y_T)'\\\\\\\\\n",
    "\\underbrace{X}_{(T-1) \\times 3} &= \\begin{pmatrix} 1 & y_1 & \\epsilon_1 \\\\ ... & ... \\\\ 1 & y_{T-1} & \\epsilon_{T-1} \\end{pmatrix} \\\\\\\\\n",
    "\\underbrace{\\epsilon}_{(T-1)\\times 1} &= (\\epsilon_2, ..., \\epsilon_{T})' \\, :\\sim iid (0,\\sigma^2_{\\epsilon})\\\\\\\\\n",
    "\\underbrace{\\beta}_{3 \\times 1} &= (\\phi_0, \\phi_1, \\theta_1)'.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Ordinary Least Squares Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## C.1 Estimating Parameters and Innovations of LRMs using OLS\n",
    "Now we will take a look on how to use the estimation technique of OLS. \n",
    "\n",
    "\n",
    "\n",
    "We work with the linear regression model\n",
    "\n",
    "$$\n",
    "y = X\\beta + \\epsilon, \\; \\epsilon \\sim iid(0,\\sigma^2_{\\epsilon}I).\n",
    "$$\n",
    "\n",
    "Whether OLS is applicable to estimate $\\beta$ and $\\sigma^2_{\\epsilon}$ depends on the data characteristics of $y$ and $X$. OLS is the method of choice if the data shares the following features\n",
    "\n",
    "* $X$ is measured without measurement error and can hence be treated as being a constant. Technically, this assumption is called **'Weak exogeneity**' assumption.\n",
    "\n",
    "\n",
    "* $y$ and $X$ share a linear relationship; also called **'linearity assumption'**. Notice, due to 'weak exogeneity', $X$ is treated as a non-random constant and hence any form of deterministic transformation (such as a polynominal etc) can be applied to members of $X$. \n",
    "\n",
    "\n",
    "* The innovations are **White Noise**.\n",
    "\n",
    "\n",
    "* There is **no multi-colinearity** between the different columns of $X$. That says, no covariate is a linear function of the other co-variates.\n",
    "\n",
    " \n",
    "Notice, OLS provides only **unbiased and consistent** estimates for $\\beta$, $\\sigma^2_{\\epsilon}$ and $\\epsilon$ if the data at hand shares the above mentioned four features. If your data violates at least one of these features, and you still estimate the unkonwn parameters by OLS, your resulting investment and risk management advice is likely misguided.\n",
    "\n",
    "\n",
    "\n",
    "The OLS parameter estimates take the well known form\n",
    "\n",
    "\\begin{align*}\n",
    " \\hat{\\beta}_{ols} :&= (X' X)^{{}-1} X' y \\\\\\\\\n",
    " \\hat{\\epsilon}_{ols} :&= y - X\\hat{\\beta}_{ols} \\\\\\\\\n",
    " \\hat{\\sigma}^2_{\\epsilon, ols}:&= \\frac{1}{T-p-1} \\; \\hat{\\epsilon}_{ols}' \\, \\hat{\\epsilon}_{ols} \\\\\\\\\n",
    " \\hat{var}_{ols}[\\beta] :&= \\hat{\\sigma}^2_{\\epsilon,ols}\\, \\times \\, (X' X)^{{}-1}  \\\\\\\\\n",
    " \\hat{s.e.}_{ols}[\\beta_i] :&= \\sqrt{[\\hat{var}_{ols}[\\beta]]_{[i,i]}} \\text{ for } i \\in [0,1,...,p] \\\\\\\\\n",
    " \\hat{t}_{ols}[\\beta_i] :&= \\frac{[\\hat{\\beta}_{ols}]_{[i,0]}}{\\hat{s.e.}_{ols}[\\beta_i]} \\\\\\\\\n",
    " \\hat{R}^2_{ols} :&= 1 - \\frac{(T-p-1) \\times \\hat{\\sigma}^2_{\\epsilon,ols} }{(T-1) \\times \\sigma^2_y }, \\text{where } \\sigma^2_y := \\frac{\\sum_{t=1}^T (y_t-\\bar{y})^2}{T-1}, \\text{where $\\bar{y}$ is the sample mean of $Y$} \\\\\\\\\n",
    "  \\hat{\\bar{R}}^2_{ols} :&= 1 - \\frac{\\hat{\\sigma}^2_{\\epsilon,ols}}{\\sigma^2_y} \\\\\\\\\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C.2 Deriving Analytical OLS Parameter Estimates for LRMs\n",
    "\n",
    "Understanding this derivation is essential for doing quant research or working in a quant job. It makes you aware that you need to think about your objective, how to translate it to math and how to come up with the optimal solution. \n",
    "\n",
    "\n",
    "\n",
    "Ask yourself, what is it that a linear regression model tries to achieve? Well, it wants to project $y$ onto $X$ such that the sum of squared errors is minimal. The word 'minimal' highlights that we will write down our objective function (sum of squared errors) and set the first-order condition (with regard to $\\beta$) to zero. Solving for the resulting $\\beta$ gives us the optimal $\\hat{\\beta}_{ols}$. Then, we ask whether the ols solution for 'beta' is unbiased. That means we ask whether the expected value of the ols solution for beta does coincide with the population beta. Next, we quantify how much trust we have in the estimated beta. Doing so means we determine the variance of the ols solution for beta. The higher the variance the less certain can we be that the ols estimate for beta is close to the population beta. We can spin that further by determining the ratio of the ols solution for beta and the square-root of its variance. That ratio is called t-statistic. Last not least, we will ask how much of the variance in $y$ is explained by the variance of $X\\hat{\\beta}_{ols}$ and the respective metric will be the $R^2$\n",
    "\n",
    "\n",
    "So, lets go step by step. The **OLS Objective Function is:**\n",
    "\n",
    "\\begin{align*}\n",
    " \\hat{\\beta}_{ols} &:= arg\\min_{\\beta} \\; (y-X\\beta)' \\, (y-X \\beta),\n",
    "\\end{align*}\n",
    "\n",
    "The last equation says that we want to find the linear slope coefficient $\\beta$ such that the average squared prediction error is minimal.\n",
    "\n",
    "\n",
    "\n",
    "Notice, the OLS objective function is also called 'L2-minimization', because we try to find a linear mapping that minimizes the sum of quadratic fitting errors. Importantly, for OLS, each fitting error gets the same weight in the objective function. This is the result of the assumption that each realized $y_t$ is as informative about $\\beta$; a direct (and important) consequence of $\\epsilon_t:\\sim i.i.d(0,\\sigma^2_{\\epsilon}), \\forall t>0$. Mathematically, we can highlight this insight by re-writing the OLS objective function as \n",
    "\n",
    "$$\n",
    " \\hat{\\beta}_{ols} := arg\\min_{{\\beta}} \\; \\sum_{t=1}^T \\; w_t \\times (y_t - {\\beta}'\\;x_t)^2; \\quad \\; w_t \\equiv 1 \\, \\forall t \\in \\{1,...,T\\}.\n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\\\ \n",
    "$$\n",
    "\n",
    "Let me remark the following: Applications with $w_t \\neq 1$ are called 'weighted least squares' problems. Such applications are confronted with data that does not share the same informativeness about the unknown $\\beta$. A higher $w_t$ overweights observation $t$ as this observation is particularly informative about $\\beta$, which means the variance of $\\epsilon_t$ is particularly small; relative to the other observations. In contrast, time $t$ observations with relatively large error variances are down-weighted. This ends my remark.\n",
    "\n",
    "\n",
    "\n",
    "So, let's get back to the derivation of the OLS solution. As we have pinned down the OLS objective function, we now solve for the optimal slope coefficient. Verify on your own, that the OLS objective function coincides with\n",
    "\n",
    "\\begin{align*}\n",
    " \\min_{{\\beta}} \\;y'y - 2 {\\beta}' \\, X'y + {\\beta}' \\, X'X \\, \\beta. \n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "Setting the first-order condition to zero and solving it, reveals the optimal OLS estimate for the linear slope coefficient is,\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\beta}_{ols} = (X' X)^{{}-1} X' y. \n",
    "\\end{align*}\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "Notice, $ (X' X)^{{}-1}$ exists if $X$ has full rank, meaning if the columns of $X$ are linearly independent of each other. \n",
    "\n",
    "\n",
    "\n",
    "Next, the estimate for the constant variance of the iid error is \n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\sigma}^2_{\\epsilon,ols} &= \\frac{1}{T-p-1} \\, (y - X\\hat{\\beta}_{ols})' \\, (y - X\\hat{\\beta}_{ols}),  \n",
    "\\end{align*}\n",
    "\n",
    "where $p+1$ are the number of parameters that we estimated.\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "Similarly, you also get the OLS estimate for the realized sequence of the iid error term via\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\epsilon}_{ols} = y - X\\hat{\\beta}_{ols}. \n",
    "\\end{align*}\n",
    "\n",
    "$$ \n",
    "\\\\ \\\\ \n",
    "$$\n",
    "\n",
    "\n",
    "Next, we ask how confident can we be that the OLS estimate for beta coincides with the true, yet unknown, population beta. Conceptually, we want to find the variance of the OLS estimator. So, first, we write down the OLS solution for beta and do some manipulations to show the OLS beta is unbiased; then we determine its variance. Mathematically, we can write \n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\beta}_{ols} &= (X' X)^{{}-1} X' y \\nonumber \\\\ \\\\\n",
    "&=  (X' X)^{{}-1} X' (X\\beta + \\epsilon) \\\\\\\\\n",
    "&= (X' X)^{{}-1} X' X \\beta + (X' X)^{{}-1} X' \\epsilon \\nonumber \\\\\\\\\n",
    "&= \\beta + (X' X)^{{}-1} X' \\epsilon. \n",
    "\\end{align*}\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "The last equation highlights that the OLS solution for beta is the sum of the true, yet unknown population beta, and data noise. Taking the expectation on the left and right hand side gives us\n",
    "\n",
    "\n",
    "$$\n",
    "E[\\hat{\\beta}_{ols}] = \\beta + 0, \\; \n",
    "$$\n",
    "\n",
    "\n",
    "because of the implied OLS assumption that $E[X'\\epsilon]=0$.\n",
    "\n",
    "\n",
    "\n",
    "The last equation says that the OLS estimate for $\\beta$ is unbiased if the $X$ variables are uncorrelated with $\\epsilon$. This holds if the 'weak exogeneity' assumption holds. From that you also learn that if $X$ is random with a non-zero correlation with $\\epsilon$, the OLS estimate is biased.\n",
    "\n",
    "\n",
    "\n",
    "We can re-write the last equation as\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\beta}_{ols} - E[\\hat{\\beta}_{ols}] & = (X' X)^{{}-1} X' \\epsilon \\quad \\; \\rightarrow \\\\\\\\\n",
    "var(\\hat{\\beta}_{ols}) &= E\\left[(X' X)^{{}-1} X' \\epsilon \\, \\epsilon' \\, X (X' X)^{{}-1} \\right] \\\\ \\\\\n",
    "&= (X' X)^{{}-1} X' \\; \\sigma^2_{\\epsilon} I \\; X (X' X)^{{}-1} \\\\\\\\\n",
    "&= \\sigma^2_{\\epsilon} \\, \\times \\, (X' X)^{{}-1},\n",
    "\\end{align*}\n",
    "\n",
    "which crucially relied upon the assumption that $\\epsilon_t :\\sim i.i.d.(0,\\sigma^2_{\\epsilon}) \\forall t>0$ which implies $E[\\epsilon \\epsilon'] = \\sigma^2_{\\epsilon}\\,I$ and which coincides with the assumptions that the error terms are White Noise. That also teaches you that the precision of the OLS solution is overstated if your errors in data are auto-correlated or heteroscedastic.  \n",
    "\n",
    "\n",
    "\n",
    "The data estimate for $var(\\hat{\\beta}_{ols}) $ is therefore\n",
    "\n",
    "$$\n",
    "\\hat{var}_{ols}[\\beta] = \\hat{\\sigma}^2_{\\epsilon,ols}\\, \\times \\, (X' X)^{{}-1}.\n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\\\ \\\\\n",
    "$$\n",
    "\n",
    "Next, we turn to the OLS Standard Error, t-stat and $R^2$.  \n",
    "\n",
    "\n",
    "The standard error of an estimate is simply the square-root of the variance of the respective parameter estimate, i.e.\n",
    "\n",
    "$$\n",
    "\\hat{s.e.}_{ols}[\\beta_i] := \\sqrt{[\\hat{var}_{ols}[\\beta]]_{[i,i]}},\\quad \\forall i \\in [0,1,...,p]\n",
    "$$\n",
    "\n",
    "Notice, the standard error for $\\hat{\\sigma}^2_{\\epsilon}$ cannot be determined in the OLS set-up. It remains unidentified.\n",
    "\n",
    "\n",
    "\n",
    "The t-value (i.e. t-statistic) for an estimated coefficient coincides with the ratio of point estimate over its standard error. Hence,\n",
    "\n",
    "$$\n",
    "\\hat{t}_{ols}[\\beta_i] := \\frac{[\\hat{\\beta}_{ols}]_{[i,1]}}{\\hat{s.e.}_{ols}[\\beta_i]}, \\quad \\; \\forall i \\in [0,...,p].\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\\\ \\\\ \n",
    "$$\n",
    "\n",
    "With regard to the $R^2$ note that\n",
    "\n",
    "\\begin{align*}\n",
    "R^2 &:= \\frac{var(X \\beta)}{var(y)} \\\\\\\\\n",
    "& = \\frac{var(y)-var(y-X \\beta)}{var(y)} \\nonumber \\\\\\\\\n",
    "& = 1-\\frac{var(y-X \\beta)}{var(y)} \\\\\\\\\n",
    "&= 1 - \\frac{(y-X\\hat{\\beta})'(y-X\\hat{\\beta})}{(y-\\bar{y})'(y-\\bar{y})}.  \n",
    "\\end{align*}\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "The more variables you add to $X$ the higher the $R^2$. Of course, it does not make sense to add any variable just to increase the $R^2$. If $X$ is multidimensional it is advisable to report $\\bar{R^2}$ (adjusted $R^2$),\n",
    "\n",
    "\\begin{align*}\n",
    "\\bar{R}^2 &:=   1-R^2 \\times \\frac{T-1}{T-p-1} \\\\\\\\\n",
    "&=  1 - \\frac{\\hat{\\sigma}^2_{\\epsilon,ols}}{\\sigma^2_y},\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "where \n",
    "\\begin{align*}\n",
    "\\sigma^2_y := \\frac{\\sum_{t=1}^T (y_t-\\bar{y})}{T-1}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:42:20.974692Z",
     "start_time": "2025-06-19T12:42:20.971469Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D. OLS for ARMA(p,q) Models\n",
    "\n",
    "For $q=0$, i.e. (AR(p) Models), OLS is the method of choice. This holds because both endogenous and exogenous variables are observed to the data scientists and their relationship is linear. Note, if you doubt that the errors have a constant volatility, which means you doubt they are homoscedastic, variations of OLS exist. Such variations are weighted least squares (WLS) and generalized least squares (GLS). Here, I do not talk about these variations.\n",
    "\n",
    "\n",
    "## D.1 OLS for AR(p) Models\n",
    "\n",
    "Write the AR(p) as a LRM and apply the above OLS formulas to get the parameter estimates, their t-stat, the r-square and time series of innovations.\n",
    "\n",
    "## D.2 2-Pass OLS for ARMA(p,q) Models\n",
    "\n",
    "Data science, and empirical work in general, does often follow a \"divide and conquer strategy\". 2-Pass OLS for ARMA(p,q) models is an example for such a strategy. The problem is that the q-MA terms of lagged values of innovations are not weakly exogenous. So, one assumption for OLS is not valid. 2-Pass estimation is one way around the problem. The procuedure is a sequential two step OLS approach. \n",
    "\n",
    "First, fit the AR(p) via OLS and record the time-series of residuals. This identifies the AR(p) parameters. \n",
    "\n",
    "\n",
    "Second, fit an AR(q) to the time-series of residuals to identify the MA(q) parameters.  \n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "While these AR(p) and MA(q) estimates are only approximations, that approach is practical, quick and pretty accurate. If you want the maximum accuracy possible, you can use these values as starting values for a subsequent Maximum Likelihood Estimation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D.3 Example: 2-Pass Estimation of ARMA(1,1) Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ARMA(1,1) for $y$ reads:\n",
    "\n",
    "$$\n",
    "y_t := \\phi_0 + \\phi_1 y_{t-1} + \\theta_1 \\epsilon_{t-1} + \\epsilon_t, \\; \\epsilon_t :\\sim iid(0,\\sigma^2_{\\epsilon}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pass 1: AR(p) Parameters** \n",
    "\n",
    "Fit AR(1) model to $y$ to get $\\phi_0, \\phi_1$ and $\\{\\epsilon_t\\}_t$.\n",
    " \n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "**Pass 2: MA(q) Parameters**\n",
    "\n",
    "Fit an AR(q) to $\\{\\epsilon_t\\}_t$ from pass 1, i.e.\n",
    "\n",
    "\\begin{align*}\n",
    "y = X\\beta +  \\nu \\\\\\\\\n",
    "\\underbrace{y}_{(T-2)\\times 1} &= (\\epsilon_3, ..., \\epsilon_T)'\\\\\\\\\n",
    "\\underbrace{X}_{(T-2) \\times 1} &= \\begin{pmatrix} \\epsilon_2 \\\\ ...  \\\\ \\epsilon_{T-1} \\end{pmatrix} \\\\\\\\\n",
    "\\underbrace{\\nu}_{(T-2)\\times 1} &= (\\nu_3, ..., \\nu_{T})' \\, :\\sim iid (0,\\sigma^2_{\\epsilon})\\\\\\\\\n",
    "\\underbrace{\\beta}_{1 \\times 1} &= (\\theta_1).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:42:20.997631Z",
     "start_time": "2025-06-19T12:42:20.993452Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E. Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "## E.1 MLE: A Bird's Eye View\n",
    "\n",
    "Now we want to take a bird's eye view to explain the concept of learning unknown parameters by the method of Maximum Likelihood. We talk about 'what is the log-likelihood function of a model', 'how to find the maximum likelihood estimate', and 'why is MLE such a big thing'.  \n",
    "\n",
    "\n",
    "\n",
    "Assume you observe a time series of returns\n",
    "\n",
    "$$\n",
    "r_1, r_2, ..., r_T.\n",
    "$$\n",
    "\n",
    "Further assume you want to explain that return time series with a specific parametric model that depends on a parameter vector $\\theta$ and that implies a joint probability density function $f$ which reads\n",
    "\n",
    "$$\n",
    "f(r_{T},\\ r_{T-1},\\ \\cdots,\\ r_{2},\\ r_{1};\\theta).\n",
    "$$\n",
    "\n",
    "Any joint probability density function is simply the product of conditional densities and one marginal density, i.e.\n",
    "\n",
    "$$\n",
    "f(r_{T},\\ r_{T-1},\\ \\cdots,\\ r_{2},\\ r_{1};\\theta)\n",
    "=\\left[ \\prod_{t=2}^{T}f(r_{t}|r_{t-1},\\ \\cdots,\\ r_{1};\\theta) \\right] \\times f(r_{1};\\theta).  \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "Let's look at two examples that you often come across in finance research. First, if all returns are independent of each other, the joint likelihood reads\n",
    "\n",
    "$$\n",
    "f(r_{T},\\ r_{T-1},\\ \\cdots,\\ r_{2},\\ r_{1};\\theta) \\overbrace{=}^{indep} \\,  \\left[ \\prod_{t=1}^{T}f(r_{t};\\theta) \\right].\n",
    "$$\n",
    "\n",
    "Second, if returns follow a Markov structure, it means that todays return depends on yesterdays return, than the joint likelihood coincides with \n",
    "\n",
    "$$\n",
    "f(r_{T},\\ r_{T-1},\\ \\cdots,\\ r_{2},\\ r_{1};\\theta) \\overbrace{=}^{Markov} \\left[ \\prod_{t=2}^{T}f(r_{t}|r_{t-1};\\theta) \\right]\\times f(r_{1};\\theta).\n",
    "$$\n",
    "\n",
    "Estimating a model with the method of Maximum Likelihood means to find the parameter vector $\\theta$ that maximizes the joint probability function. In order to highlight that this joint likelihood function depends on $\\theta$, we often write it as \n",
    "\n",
    "$$\n",
    "L(\\theta|\\{r_t\\}_{t \\in [1,...,T]}) := f(r_{T},\\ r_{T-1},\\ \\cdots,\\ r_{2},\\ r_{1};\\theta), \n",
    "$$ \n",
    "\n",
    "or in short, simply $L(\\theta)$. \n",
    "\n",
    "\n",
    "\n",
    "The value of $\\theta$ that maximizes $L(\\theta)$ does also maximize $\\ln L(\\theta)$, because the log-function is a strictly positive monotone transform. Numerically, optimizing the log likelihood results in a more stable routine.\n",
    "\n",
    "\n",
    "\n",
    "I am going to provide a big picture overview of how to find the $\\theta$ that maximizes the log likelihood function. \n",
    "\n",
    "\n",
    "\n",
    "There are three classical optimization approaches to find the maximum likelihood estimate $\\hat{\\theta}^{ML}$.\n",
    "\n",
    "**Analytic Optimization:** If $\\ln L(\\theta)$ is given as an analytical expression, one can calculate the gradient (first order condition w.r.t $\\theta$), set it to zero and solve the resulting equation. If the second derivative, evaluated at $\\hat{\\theta}$ is negative, you know that $\\hat{\\theta}$ maximizes the log-likelihood.\n",
    "\n",
    "\n",
    "**Grid Search:** This approach also works but might not always be applicable due to its inefficiency. If you know that $\\theta$ lies in a subset, you can do an uninformed search within that set. Basically, evaluate $\\ln L(\\theta)$ for all $\\theta$ in the subset and pick the one that produces the largest log likelihood value. That approach is applicable if $\\theta$ is one-dimensional and if the subset is small.\n",
    "\n",
    "\n",
    "**Numerical Optimization:** This is basically the smart way of performing a search. There are local and global optimization routines. The local optimization routine starts from a starting value for $\\theta$ and goes uphill until it finds a plateau. How to go uphill? Well, based on the start value, you determine the first derivative of the log likelihood with regard to the specific value of $\\theta$. If the derivative is positive, it means increasing the value of $\\theta$ a little bit is going to increase the log likelihood. The plateau that the local optimizer finds is usually only a local maximum. Very importantly!! You want to find the global maximum. So either, you have to run the local optimizer with very many different starting values or use a global optimizer. There are different global optimizers. Most, if not all of them, contain a stochastic component that allows to jump by chance from one plateau to another.\n",
    "\n",
    "\n",
    "\n",
    "I now focus on the last item of this part: 'What are the statistical properties of the MLE'? We distinguish whether the sample is small or large and start with the large sample properties.\n",
    "\n",
    "\n",
    "\n",
    "In large samples, the MLE estimator for $\\theta$ is \n",
    "\n",
    "* First, Consistent; meaning that the plim $\\hat{\\theta}_{ML}=\\theta$ which says that as the sample converges to an infinite length, the MLE estimate for $\\theta$ gives you the correct population parameter and unbiased.\n",
    "\n",
    "\n",
    "* Second, Asymptotically Normally Distributed; meaning that $\\hat{\\theta}_{ML}\\sim N[\\theta,\\ \\{I(\\theta)\\}^{-1}]$, where $I(\\theta)$ is the Information Matrix.\n",
    "\n",
    "\n",
    "* Third, the Variance of the MLE fulfills the Rao-Cramer lower bound, which says that it is the most efficient unbiased estimate. 'Most efficient' means it comes along with the smallest variance.\n",
    "\n",
    "\n",
    "* Invariance: If $\\theta_{ML}$ is the ML estimator of $\\theta$, then $g(\\theta_{ML})$ is the maximum likelihood estimator of $\\gamma=g(\\theta)$ . This means that rather than estimating a parameter $\\theta$, we can instead estimate some function of it, $g(\\theta)$ and recover the MLE of $\\theta$ from inverting $g(\\theta)$.\n",
    "\n",
    "\n",
    "Let's turn to the 'small sample properties of MLE'.\n",
    "\n",
    "\n",
    "\n",
    "This might come as a surprise, but the small sample properties of the ML estimator are not well understood (i.e. the estimate might be biased and the standard errors might not be well identified based on the inverse of the information matrix). I know that is shocking! Yet, you should take away that you need at least 100 data points to make informed decisions with MLE. \n",
    "\n",
    "\n",
    "\n",
    "As a final note: If you apply MLE on a sample of less than 100 observations, you could try to get the standard errors by **'bootstrapping'**. The idea is as follows: Assume we want to obtain a bootstrap estimate of the standard error of $\\hat{\\theta}$. Suppose we have 500 random samples from the population. From that we could get 500 different estimates of $\\hat{\\theta}$ and calculate the standard error of $\\hat{\\theta}$ as the standard deviation of these 500 estimates.\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "## E.2 Example: Analytical MLE for Poisson Distributed Random Variables\n",
    "\n",
    "Now we will guide you through one MLE example.\n",
    "\n",
    "\n",
    "\n",
    "Let $y_t$ be the number of days in year $t$ where the SP500 lost more than 10\\%. A starting model for such crash events might be to assume that such days occur according to a Poisson distribution with intensity parameter $\\lambda$. So, the probability of observing in a year five days with a negative return of more than 10\\% would be\n",
    "\n",
    "$$\n",
    "Prob(y_t=5;\\theta) \\overbrace{=}^{Pois} \\, \\frac{e^{-\\lambda} \\lambda^{5}}{5!}.\n",
    "$$\n",
    "\n",
    "OK: now we want to apply MLE to estimate the intensity parameter $\\lambda$. Assume, you have recorded for the last 100 years the number of days per year at which the SP500 lost at least 10\\% of its value, $\\{y_1, y_2, ..., y_{100}\\}$. The joint likelihood equals \n",
    "\n",
    "\\begin{align*}\n",
    "L(\\lambda) & = \\frac{e^{-\\lambda}\\lambda^{y_{1}}}{y_{1}!}\\times\\frac{e^{-\\lambda}\\lambda^{y_{2}}}{y_{2}!}\\times\\ldots\\times\\frac{e^{-\\lambda}\\lambda^{y_{T}}}{y_{T}!} \\\\\n",
    "&= \\prod_{t=1}^{T}\\frac{e^{-\\lambda}\\lambda^{y_{t}}}{y_{t}!}.\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "The $\\log$ likelihood is\n",
    "\n",
    "$$\n",
    "\\ln L(\\lambda) \\ =\\  -T\\lambda\\ +\\sum_{t=1}^T y_{t}\\ln(\\lambda)-\\sum_{t=1}^T \\ln(y_{t}!)\n",
    "$$\n",
    "\n",
    "The MLE of $\\lambda$ maximizes $\\ln L(\\lambda)$; hence leads to a zero gradient, i.e.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\ln L(\\lambda)}{\\partial\\lambda}=-T+\\frac{\\sum_{t=1}^{T}y_{t}}{\\lambda} \\overbrace{=}^{!} 0\n",
    "$$\n",
    "\n",
    "We solve for $\\lambda$:\n",
    "\n",
    "$$\n",
    "\\hat{\\lambda}_{ML}\\ =\\ \\frac{\\sum_{t=1}^{T}y_{t}}{T}.\n",
    "$$\n",
    "\n",
    "and check that the second derivative is negative (I leave that for you as an exercise).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "So, in words: the MLE for the crash intensity factor is simply the average number of crash days in a year.\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "## E.3 MLE for a Gaussian LRM\n",
    "\n",
    "### E.3.1 Gaussian LRM\n",
    "\n",
    "Let's consider the linear regression problem with Gaussian regression error:\n",
    "\n",
    "$$\n",
    "y = X \\beta + \\epsilon; \\; \\epsilon \\sim N(0,\\sigma^2 I)\n",
    "$$ \n",
    "with $dim(y) = T\\times 1$ and $dim(X) = T\\times k$ and $dim(\\beta) = k \\times 1$.\n",
    "\n",
    "\n",
    "### E.3.2 Deriving the Log Likelihood\n",
    "\n",
    "This part derives the joint log likelihood of the Gaussian Linear Regression Model.  Such a model take the form\n",
    "\n",
    "$$\n",
    "y = X \\beta + \\epsilon; \\; \\epsilon \\sim N(0,\\sigma^2 I)\n",
    "$$ \n",
    "\n",
    "with $dim(y) = T\\times 1$ and $dim(X) = T\\times k$ and $dim(\\beta) = k \\times 1$.\n",
    "\n",
    "\n",
    "\n",
    "Notice, the likelihood for a single observation $y_i$ is:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(y_{i}|x_i,\\ \\beta,\\ \\sigma^{2})=\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}e^{-\\frac{(y_{i}-x_{i}\\beta)^{2}}{2\\sigma^{2}}}\n",
    "$$\n",
    "\n",
    "where $dim(x_i) = 1 \\times k$.\n",
    "\n",
    "\n",
    "\n",
    "As $\\epsilon$ is an iid process, the joint likelihood for the whole sample $y$ is simply the product of all single likelihoods. We can write that as follows:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(y|X,\\ \\beta,\\ \\sigma^{2})=\\prod_{i=1}^{T}\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}e^{-\\frac{(y_{i}-x_{i}\\beta)^{2}}{2\\sigma^{2}}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "Now taking logs, the $\\log$ likelihood is:\n",
    "\n",
    "$$\n",
    "\\ln \\mathcal{L}(y|X,\\ \\beta,\\ \\sigma^{2})\\ =\\ \\sum_{i=1}^{T}\\ln \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}e^{-\\frac{(y_{i}-x_{i}\\beta)^{2}}{2\\sigma^{2}})}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\\ -\\frac{T}{2}\\ln(2\\pi)-\\frac{T}{2}\\ln(\\sigma^{2})-\\frac{1}{2}\\sum_{i=1}^{T}\\left[ \\frac{(y_{i}-x_{i}\\beta)^{2}}{\\sigma^{2}} \\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "In matrix form, the $\\log$ likelihood is:\n",
    "\n",
    "$$\n",
    "\\ln \\mathcal{L}=\\frac{-T}{2}\\ln(2\\pi)-\\frac{T}{2}\\ln(\\sigma^{2})-\\frac{1}{2\\sigma^{2}}(y-X\\beta)'(y-X\\beta)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "So, we have accomplished the objective of this part. The log likelihood function is derived. If you look carefully at the last expression you see that the parameter vector $\\beta$ enters the log likelihood function only via the last term. Moreover, that term looks like the objective function to an OLS problem. That is because in order to maximize that term you have to find a $\\beta$ vector that minimizes the sum of weighted squared prediction errors. The weight is constant across all time periods, here it is $\\frac{1}{2 \\sigma^2}$. Considering that, it will not come as a surprise, that the ML estimate for $\\beta$ coincides with the OLS estimate for $\\beta$.\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "### E.3.3 Optimizing $ln (L)$\n",
    "\n",
    "In this part, I am showing to step by step how to analytically maximize the log likelihood function of the Gaussian linear regression model to find the ML estimates.   \n",
    "\n",
    "\n",
    "\n",
    "In a previous video, I have verified that the respective joint log likelihood function takes the form\n",
    "\n",
    "$$\n",
    "\\ln \\mathcal{L}=\\frac{-T}{2}\\ln(2\\pi)-\\frac{T}{2}\\ln(\\sigma^{2})-\\frac{1}{2\\sigma^{2}}(y-X\\beta)'(y-X\\beta)\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "We want to find the ML estimate for $\\theta \\equiv \\{\\beta, \\sigma^2\\}$.\n",
    "\n",
    "\n",
    "\n",
    "By expanding the numerator of the last term and moving the scalar $\\sigma^{2}$ outside, we see that the joint log likelihood function equals:\n",
    "\n",
    "$$\n",
    "\\ln \\mathcal{L}(y|X,\\ \\beta,\\ \\sigma^{2})\\ =\\ -\\frac{T}{2}\\ln(2\\pi)-\\frac{T}{2}\\ln(\\sigma^{2})-\\frac{1}{2\\sigma^{2}}[y'y-2y'X\\beta+\\beta'X'X\\beta]\n",
    "$$\n",
    "\n",
    "To find the gradient vector, we need to take the derivative of $\\ln \\mathcal{L}$ with respect to $\\beta$ and $\\sigma^{2}$.\n",
    "\n",
    "\n",
    "Let's start by taking the derivative with respect to $\\beta$.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\ln \\mathcal{L}}{\\partial\\beta}\\ =\\ -\\frac{1}{2\\sigma^{2}}\\left[ \\frac{\\partial[y'y-2y'X\\beta+\\beta'X'X\\beta]}{\\partial\\beta} \\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\\ -\\frac{1}{2\\sigma^{2}}[-2X'y+2X'X\\beta]\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\\ \\frac{1}{2\\sigma^{2}}[2X'y-2X'X\\beta]\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\\ \\frac{1}{\\sigma^{2}}[X'y-X'X\\beta]\n",
    "$$\n",
    "\n",
    "We now set this equal to zero:\n",
    "\n",
    "$$ \n",
    "\\frac{1}{\\sigma^{2}}[X'y-X'X\\beta] = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    " X'X\\beta = X'y\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = (X'X)^{-1}X'y\n",
    "$$\n",
    "\n",
    "\n",
    "Hey, wait a minute; isn't that the OLS solution to the problem? ... Yes, it is!! As you can see, OLS and ML share the same parameter estimator for $\\beta$when you estimate a linear model with Gaussian White Noise. I highlight, this depends crucially on the Gaussian error and on the linear model set-up. \n",
    "\n",
    "\n",
    "\n",
    "Second, we take the derivative of with respect to $\\sigma^{2}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\ln \\mathcal{L}}{\\partial\\sigma^{2}}\\ =\\ -\\frac{T}{2\\sigma^{2}}+\\frac{1}{2\\sigma^{4}} \\left[(y-X\\beta)'(y-X\\beta)\\right]\n",
    "$$\n",
    "\n",
    "We now set this equal to zero:\n",
    "\n",
    "$$\n",
    "-\\frac{T}{2\\sigma^{2}}+\\frac{1}{2\\sigma^{4}} \\left[(y-X\\beta)'(y-X\\beta) \\right]\\ =\\ 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{1}{2\\sigma^{4}} \\left[(y-X\\beta)'(y-X\\beta)\\right]\\ =\\ \\frac{T}{2\\sigma^{2}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\sigma^{2}} \\left[(y-X\\beta)'(y-X\\beta) \\right]\\ =\\ T\n",
    "$$\n",
    "\n",
    "\n",
    "Since we have already solved for $\\hat{\\beta}$, we can solve for $\\sigma^{2}$ by replacing $\\beta$ with its estimate:\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\sigma^{2}}\\left[(y-X\\hat{\\beta})'(y-X\\hat{\\beta})\\right]\\ =\\ T\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\sigma^{2}}\\left[(y-\\hat{y})'(y-\\hat{y}) \\right]\\ =\\ T\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\sigma^{2}}[e'e]\\ =\\ T\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^{2}=\\frac{e'e}{T}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "If you compare the ML estimate with the unbiased OLS estimate for $\\sigma^2$, $\\frac{e'e}{T-k}$, you notice that both estimators coincide for large samples but differ in small samples. In fact, the MLE estimate of $\\sigma^2$ is downward biased in small samples. \n",
    "\n",
    "\n",
    "I want to end this with a short summary. For the case of a linear Gaussian regression model, we have learned the following. First, the joint optimization over $\\beta$ and $\\sigma^2$ can be split into two separate optimizations. You start with optimizing over the slope parameter $\\beta$. Once, you have found the ML estimate for $\\beta$, you find the $\\sigma^2$ that maximizes the joint log-likelihood function. In that step, swap the population beta with the ML estimate for $\\beta$. \n",
    "\n",
    "The second learning point is that the log-likelihood function of the Gaussian linear model takes a convenient parametric expression. In fact it is so convenient, that we can find the ML estimates analytically. \n",
    "\n",
    "Third, the ML estimates for $\\beta$ coincide with the OLS solution. \n",
    "\n",
    "Fourth, the ML estimate for the variance term is downward biased in small samples but consistent. \n",
    "\n",
    "Fifth, even if the model was non-linear in $\\beta$ but with a Gaussian White Noise error, the ML solution for $\\sigma^2$ will coincide with $\\frac{e'e}{T}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:42:21.023381Z",
     "start_time": "2025-06-19T12:42:21.020122Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H. Python Visualizations and Coding for OLS and MLE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H.2. From Scratch: Estimate AR(3) for Rates with OLS and MLE\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:42:21.053512Z",
     "start_time": "2025-06-19T12:42:21.047317Z"
    }
   },
   "source": [
    "# Packages\n",
    "\n",
    "# econometric work\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "#  optimization\n",
    "import scipy.optimize as sco\n",
    "\n",
    "# numerical differentiation  \n",
    "import numdifftools as nd\n",
    "\n",
    "# plotting  \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use('ggplot')"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:42:21.158660Z",
     "start_time": "2025-06-19T12:42:21.074775Z"
    }
   },
   "source": [
    "# Data: Term Structure of Interest Rates\n",
    "\n",
    "data = pd.read_excel('data_AR3.xls', 'Rates')\n",
    "data = data.set_index('Date')\n",
    "\n",
    "# Load 3m yield, 1954 - 2006\n",
    "r = data[3]\n",
    "\n",
    "#delete what is not needed anymore\n",
    "del data\n",
    "\n",
    " "
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:42:21.191741Z",
     "start_time": "2025-06-19T12:42:21.182494Z"
    }
   },
   "source": [
    "# Prepare y and X for AR(3) model\n",
    "\n",
    "# Create lagged data\n",
    "nobs = len(r)\n",
    "rLag3 = r[0:nobs-3]\n",
    "rLag2 = r[1:nobs-2]\n",
    "rLag1 = r[2:nobs-1]\n",
    "\n",
    "# X and y\n",
    "y_df = r[3:nobs]\n",
    "y    = np.matrix(y_df)\n",
    "\n",
    "ones = np.ones(nobs-3)\n",
    "X_df = pd.DataFrame(ones)\n",
    "X_df[\"rLag1\"] = rLag1.values\n",
    "X_df[\"rLag2\"] = rLag2.values\n",
    "X_df[\"rLag3\"] = rLag3.values\n",
    "X             = np.matrix(X_df)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:42:21.230280Z",
     "start_time": "2025-06-19T12:42:21.217027Z"
    }
   },
   "source": [
    "# OLS without statsmodels package\n",
    "\n",
    "y = y.reshape(622,1)\n",
    "\n",
    "# OLS parameter estimate\n",
    "xpxi = (X.getT() * X).getI()\n",
    "Bhat = xpxi * (X.getT() * y) #const, rlag1, rlag2, rlag3\n",
    "\n",
    "# prediction error  \n",
    "ehat = y - X * Bhat\n",
    "\n",
    "# vol of prediction error\n",
    "std_e = np.sqrt(1 / (len(y) - 4) * (ehat.getT() * ehat) )\n",
    "\n",
    "# \n",
    "# s.e.(OLS parameter estimate)\n",
    "se_Bhat = np.sqrt((std_e**2)[0,0] * (X.getT()*X).getI()).diagonal()\n",
    " \n",
    "# Print parameters and corresponding standard errors\n",
    "param_ols = pd.DataFrame(Bhat)\n",
    "param_ols.columns = [\"OLS_Betas\"]\n",
    "param_ols[\"OLSE_se\"] = se_Bhat.reshape(4,1)\n",
    "print(np.round(param_ols,4))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   OLS_Betas  OLSE_se\n",
      "0     0.0868   0.0355\n",
      "1     1.3938   0.0394\n",
      "2    -0.6120   0.0639\n",
      "3     0.2025   0.0393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\I745460\\AppData\\Local\\Temp\\ipykernel_36400\\2460557099.py:17: RuntimeWarning: invalid value encountered in sqrt\n",
      "  se_Bhat = np.sqrt((std_e**2)[0,0] * (X.getT()*X).getI()).diagonal()\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:42:23.529655Z",
     "start_time": "2025-06-19T12:42:21.256747Z"
    }
   },
   "source": [
    "# MLE without statsmodels package\n",
    "\n",
    "T = X.shape[0]\n",
    "nVars = X.shape[1]\n",
    "\n",
    "\n",
    "# Set starting values\n",
    "B_0 = np.zeros(nVars+1) #vol of prediction error is the additional parameter\n",
    "B_0[nVars] = 0.41       #start value for variance of prediction error\n",
    "\n",
    "# Define the -1 * (log) likelihood function as a function of B_0\n",
    "def _lnL(B_0):\n",
    "\n",
    "    beta_mat = B_0[0:nVars].reshape(4,1)\n",
    "    sigma2 = B_0[nVars]\n",
    "\n",
    "    # prediction error\n",
    "    e = y - (X * beta_mat)\n",
    "\n",
    "    # ln L = -T/2 * ln(2*pi) - T/2 ln(sigma2) - 1/(2*sigma2) * (y - X*beta)'(y - X*beta)\n",
    "    a1 = -(T/2) * np.log(2 * np.pi * sigma2)\n",
    "    a2 = -1/(2 * sigma2)\n",
    "    a3 = sum( np.square(e) )\n",
    "\n",
    "    return -1 * (a1 + a2 * a3)\n",
    "\n",
    "\n",
    "# Define settings for optimization\n",
    "# Note, estimator for sigma can only be positive\n",
    "\n",
    "# set lower bounds\n",
    "lb = -100 * np.ones(nVars+1)\n",
    "lb[nVars] = 0.0001 #variance is larger than 0.0001\n",
    "\n",
    "# set upper bounds:\n",
    "ub = 100 * np.ones(nVars+1)\n",
    " \n",
    "\n",
    "# Define bounds\n",
    "bnds = tuple((lb[x],ub[x]) for x in range(0, len(B_0)))\n",
    "\n",
    "# Perform optimization\n",
    "results = sco.minimize(fun = _lnL, x0 = B_0, bounds = bnds, method = 'L-BFGS-B', tol = 1e-6, options={'disp': True})\n",
    "\n",
    "#MLE estimates\n",
    "BhatMLE = results.x\n",
    "print('MLE Estimates:')\n",
    "print(BhatMLE)\n",
    "\n",
    "#MLE standard errors using Hessian and numerical differentiation\n",
    "hess = nd.Hessian(_lnL)\n",
    "hess_eval = np.matrix(hess(BhatMLE))\n",
    "se_hess = np.sqrt(np.diag(hess_eval.getI()))\n",
    "print('MLE s.e.')\n",
    "print(np.round(se_hess, 4))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE Estimates:\n",
      "[ 0.08673189  1.39378763 -0.61201052  0.20254716  0.17372774]\n",
      "MLE s.e.\n",
      "[0.0354 0.0393 0.0637 0.0392 0.0098]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:42:23.573370Z",
     "start_time": "2025-06-19T12:42:23.568772Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H.3. From Scratch: Estimate AR(3) for Rates with OLS and MLE in an Object-oriented Way"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:42:23.606194Z",
     "start_time": "2025-06-19T12:42:23.598330Z"
    }
   },
   "source": [
    "# Class OLS\n",
    "\n",
    "class OLS(object):\n",
    "\n",
    "    def __init__(self, Y, X, const = True):\n",
    "        self.Y = Y\n",
    "        self.X = X\n",
    "        \n",
    "        if (const == True):\n",
    "            self.X = np.insert(self.X, obj = 0, values = 1, axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "    def run_OLS(self, summary = True):\n",
    "        nobs,nvar = self.X.shape\n",
    "        #beta_ols\n",
    "        beta  = np.linalg.pinv( self.X.T @ self.X )  @ self.X.T @ self.Y   \n",
    "        #projection\n",
    "        y_hat = self.X @ beta\n",
    "        #residual\n",
    "        resid = self.Y - y_hat\n",
    "        #cov of estimates\n",
    "        cov =  (resid.T @ resid)[0,0] / (nobs - nvar) * np.linalg.pinv(self.X.T@self.X)\n",
    "        #s.e.(ols estimates)\n",
    "        se = np.sqrt(np.diag(cov))#.T\n",
    "        #t(ols estimates)\n",
    "        tstat = beta.T / se\n",
    "        #R^2: 1 - Sum res**2 / Sum (y - y_hat)**2\n",
    "        sum_resids = resid.T@resid\n",
    "        Y_dm = self.Y - self.Y.mean()\n",
    "        sum_y = Y_dm.T@Y_dm\n",
    "        rsqr  = 1 - ( sum_resids / sum_y )\n",
    "\n",
    "        # Set object variables\n",
    "        self.beta = beta\n",
    "        self.resid = resid\n",
    "        self.tstat = tstat\n",
    "        self.rsqr = rsqr\n",
    "\n",
    "        # Print results\n",
    "        if (summary == True):\n",
    "            print('Betas: ', beta)\n",
    "            print('t-stats: ', tstat)\n",
    "            print('R^2: ', rsqr)\n",
    "\n",
    "        else:\n",
    "            # return results as numpy arrays. The ravel function \"flattens\" array into a one-dimensional array\n",
    "            return self.beta.ravel(), self.resid.ravel(), self.tstat.ravel(), self.rsqr.ravel()\n"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:42:23.673983Z",
     "start_time": "2025-06-19T12:42:23.658886Z"
    }
   },
   "source": [
    "# test class OLS\n",
    "OLS_ = OLS(y,X, const=False)\n",
    "OLS_.run_OLS(summary = True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Betas:  [[ 0.08678418]\n",
      " [ 1.39377645]\n",
      " [-0.61199535]\n",
      " [ 0.20253518]]\n",
      "t-stats:  [[ 2.44342479 35.3878389  -9.57247795  5.1528489 ]]\n",
      "R^2:  [[0.97793633]]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:42:23.693384Z",
     "start_time": "2025-06-19T12:42:23.684128Z"
    }
   },
   "source": [
    "# Class MLE\n",
    "\n",
    "class MLE(object):\n",
    "\n",
    "    def __init__(self, Y, X, const = True):\n",
    "        self.Y = Y\n",
    "        self.X = X\n",
    "\n",
    "        if (const == True):\n",
    "             self.X = np.insert(self.X, obj = 0, values = 1, axis = 1)\n",
    "\n",
    "        self.nobs, self.nvar = self.X.shape\n",
    "\n",
    "        \n",
    "    def _likelihood(self, param): #[parameter, variance], negative_logLikelihood\n",
    "        # Assign param to variables\n",
    "        sigma2 = param[self.nvar]\n",
    "        beta  = param[0:self.nvar].reshape(self.nvar,1)\n",
    "        # residuals\n",
    "        resid = self.Y - (self.X @ beta)\n",
    "        # log-likelihood\n",
    "        likeli = - self.nobs * 0.5 * np.log(2*np.pi*sigma2) - 1/(2*sigma2) * sum(np.square(resid))\n",
    "        # Return -1*lnL\n",
    "        return -likeli\n",
    "\n",
    "\n",
    "    def estimate(self):\n",
    "        # Define optimization settings \n",
    "        init = np.ones(self.nvar+1)\n",
    "        # lower bounds\n",
    "        lb = -100*np.ones(self.nvar+1)\n",
    "        lb[self.nvar] = 0.001 #min value for variance of error\n",
    "        # upper bounds:\n",
    "        ub = 100*np.ones(self.nvar+1)\n",
    "        # bounds as tupel\n",
    "        bnds = tuple((lb[x],ub[x]) for x in range(0, len(init)))\n",
    "        # Perform  optimization\n",
    "        results = sco.minimize(fun = self._likelihood, x0 = init, bounds = bnds, method=\"L-BFGS-B\")\n",
    "        # store MLE estimates\n",
    "        self.sigma2 = results.x[self.nvar]\n",
    "        self.beta  = results.x[0:self.nvar]\n",
    "        # standard errors via Hessian\n",
    "        hess = nd.Hessian(self._likelihood)\n",
    "        hess_eval = np.matrix(hess(results.x))\n",
    "        se = np.sqrt(np.diag(hess_eval.getI()))\n",
    "        # t-statistics\n",
    "        self.tstat = np.divide(self.beta,se[0:self.nvar])\n",
    "\n",
    "    # get estimated parameters\n",
    "    def getResults(self, summary = True):\n",
    "        if summary:\n",
    "            print('Betas: ', self.beta)\n",
    "            print('Sigma2: ', self.sigma2)\n",
    "            print('t-stats: ', self.tstat)\n",
    "        else:\n",
    "            return self.beta.ravel(), self.sigma.ravel(), self.tstat.ravel()"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:42:26.410056Z",
     "start_time": "2025-06-19T12:42:23.719630Z"
    }
   },
   "source": [
    "# test class MLE\n",
    "MLE_ = MLE(y,X, const=False)\n",
    "MLE_.estimate()\n",
    "MLE_.getResults(summary=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Betas:  [ 0.08678141  1.39377021 -0.61198509  0.20253169]\n",
      "Sigma2:  0.17375675265331064\n",
      "t-stats:  [ 2.45124688 35.5021015  -9.60326818  5.16942068]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:42:26.433694Z",
     "start_time": "2025-06-19T12:42:26.428652Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
