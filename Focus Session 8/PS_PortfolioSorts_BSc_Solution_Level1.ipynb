{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Set \"Portfolio Sorts\" for BSc students\n",
    "\n",
    "## General Notes on this PS\n",
    "- The student is asked to go through the code thoroughly and take note of the setup and structure of the code\n",
    "- After having done this, the student should have an understanding of these following points:\n",
    "1. firstly all relevant libraries are imported\n",
    "2. After that, parameters that determine the behavior of the code and the sorting algorithm are set, all in one central place\n",
    "3. Furthermore the code employs parallelization of the computational tasks, to speed up the runtime of the code (see \"multiprocessing as mp\")\n",
    "4. The whole logic of the code is setup in an object oriented way, i.e. through a class called \"PortfolioSortsPipeline\"\n",
    "- In the last line of the code, an object of this class is created and the method (class function) \"runPipeline\" is called, to perform all steps, one after the other\n",
    "- The object oriented structure of the code enables the user to modularize/split up the code in logical units, more easily debug singular steps and most importantly reuse the code in other projects\n",
    "- Each step of the procedure is associated with one method of the class; the methods are called one after the other in the \"runPipeline\" function and are currently all commented out, so nothing will happen\n",
    "\n",
    "## Tasks for students\n",
    "- make sure you've installed all libraries needed\n",
    "- make sure you've set a path for the \"baseFolder\" variable (code line 14)\n",
    "- make sure the subfolders within your baseFolder exist\n",
    "- make sure you've placed all input data that comes with this problem set in the baseFolder path you've just specified\n",
    "- Throughout the code important steps have been removed and replaced with a block like this:\n",
    "- ########## STUDENT TASK START ##########\n",
    "- write code that does x...\n",
    "- ########## STUDENT TASK END ##########\n",
    "- For level 1, the student is asked to complete the code inbetween such lines\n",
    "- In total there are 16 (+2 for level 2) tasks in the code, the student is asked to fill them in\n",
    "- If the student feels confident about his solution, he can comment-in the corresponding function in the \"runPipeline\" method and run the pipeline\n",
    "- For example: After having completed all tasks in the function \"calcMonthlyExcessReturn\", comment-in the function in the \"runPipeline\" method and run the code\n",
    "- After having completed all tasks in the code, the code should produce two files in the \"resultsPath\" folder (code line 16):\n",
    "- PortfoliosOutcomePlusDiff.parquet which contains the time series of the sorted portfolios\n",
    "- PortfoliosOutcomePlusDiffTest.parquet which contains the statistical tests of the time series of the sorted portfolios\n",
    "- For level 2, the student is asked to complete the code for either bivariate independent or bivaraite dependent sorting and the corresponding calculation of difference and average portfolios (see functions buildSortedPortfoliosChild and calcDiffPortfolios)\n",
    "- and should describe, analyze and intepret the results in written form/presentation\n",
    "- **If you're having trouble, add logging statements and execute the code step by step, to see what's going on**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation\n",
    "\n",
    "## Import statments\n",
    "Some common libraries are used throghout this script\n",
    "\n",
    "## Parameters\n",
    "Some parameters are set, notably: <br>\n",
    "Folders that point to where the input data, the logfile and the results lie <br>\n",
    "Parameters for the parallelization, i.e. number of processes and chunk size of objects for each child process <br>\n",
    "Parameters directly relevant to the portfolio sorts algorithm, e.g. start and end date, sorting type, sortvariable names, weighing type etc. <br>\n",
    "\n",
    "## Description of data\n",
    "This problem set comes with the following input data: <br>\n",
    "SPXPrices.parquet --> Prices of the SPX, not needed <br>\n",
    "SPXConstituentsPrices.parquet --> Prices of the SPX constituents, needed <br>\n",
    "SPXConstituentsDaily.parquet --> Daily composition of the SPX, not needed <br>\n",
    "RiskFreeRatesFiltered.parquet --> Risk free rates, needed <br>\n",
    "CRAMMeasures.parquet --> Risk measures (sortvariables), needed <br>\n",
    "CompleteMapping.parquet --> ID mapping file, not needed <br>\n",
    "\n",
    "The idea is to empower the student to decide which data he needs and which preprocessing steps need to be performed before starting the actual task of sorting portfolios <br>\n",
    "\n",
    "## Code\n",
    "Everything is performed within an object of the PortfolioSortsPipeline class <br>\n",
    "The function **runPipeline()** performs all necessary steps in order <br>\n",
    "One can comment-out sub steps in this function, if needed, for example to debug <br>\n",
    "Now follows a brief description of all sub steps, which together make up the portfolio sorting algorithm <br>\n",
    "\n",
    "# calcMonthlyExcessReturn()\n",
    "This function loads prices and risk free rates from the baseFolder, where the data has to be put <br>\n",
    "It further calculates monthly excess returns for each securityid (identifyer of stock) and offsets these returns by one month, to get the one month ahead excess return, which will later be the outcome variable for the portfolio sorts <br>\n",
    "Input: SPXConstituentsPrices.parquet, RiskFreeRatesFiltered.parquet <br>\n",
    "Output: RiskFreeRatesFilteredMonthly.parquet, SPXConstituentsPricesPrepared.parquet <br>\n",
    "\n",
    "# mergeData()\n",
    "This function merges the CRAM data (measures/sortvariables) and the recently calculated one month ahead excess returns together, based on the identifiyer SecurityID <br>\n",
    "Input: SPXConstituentsPricesPrepared.parquet, CRAMMeasures.parquet <br>\n",
    "Output: MergedInputData.parquet <br>\n",
    "\n",
    "# getSortingDates()\n",
    "This function determines the first available dates each month, which will later be used for sorting and filters the input data for relevant dates and columns <br>\n",
    "Input: MergedInputData.parquet <br>\n",
    "Output: MergedInputDataFiltered.parquet, SortingDates.parquet <br>\n",
    "\n",
    "# buildSortedPortfolios()\n",
    "This function performs the sorting algorithm. It takes all relevant parameters (variables) defined above, the input data and sorting dates. <br>\n",
    "The dates are split into chunks and passed to the child processes. <br>\n",
    "As the child processes return results to the parent processes, the results are fetched and finally sorted and saved to pkl files <br>\n",
    "Input: MergedInputDataFiltered.parquet, SortingDates.parquet <br>\n",
    "Output: sortedPortfolioResults.pkl, sortedOutcomeResults.pkl, numberOfStocks.pkl, breakpointsDict.pkl, missingDataDict.pkl, missingRebalancingDatesList.pkl <br>\n",
    "\n",
    "# buildSortedPortfoliosChild()\n",
    "This is the function of the child processes, that handles the sorting <br>\n",
    "It iterates over each date in the date chunk <br>\n",
    "For each date it collects information on missing data <br>\n",
    "Breakpoints based on the sortvariable(s) are calculated. <br>\n",
    "Instruments are sorted into portfolios based on these breakpoints <br>\n",
    "Optionally, portfolios are filtered (excluded) <br>\n",
    "Weights of each stock within each portfolio (marketcap or equally weighted) and the average outcome variable per portfolio are calculated <br>\n",
    "Results are passed to the queue <br>\n",
    "\n",
    "# convertOutcome()\n",
    "This function converts the sortedOutcomeResults.pkl into a table and saves it into a parquet file. <br>\n",
    "Input: sortedOutcomeResults.pkl <br>\n",
    "Output: PortfoliosOutcome.parquet <br>\n",
    "\n",
    "# calcDiffPortfolios()\n",
    "This function calculates the diff portfolios based on the sorted portfolios depending on the type of sorting (univariate, bivariate) <br>\n",
    "Input: PortfoliosOutcome.parquet <br>\n",
    "Output: PortfoliosOutcomePlusDiff.parquet <br>\n",
    "\n",
    "# performStatTest()\n",
    "This function performs calculates the time series average and performes statistical tests on the sorted portfolio time series average outcome values. <br>\n",
    "Input: PortfoliosOutcomePlusDiff.parquet <br>\n",
    "Output: PortfoliosOutcomePlusDiffTest.parquet <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-22 20:20:34.603\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrunPipeline\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mStart Time: 2025-06-22 20:20:34\u001b[0m\n",
      "\u001b[32m2025-06-22 20:20:34.609\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrunPipeline\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mStart portfolio sorts pipeline...\u001b[0m\n",
      "\u001b[32m2025-06-22 20:20:34.715\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuildSortedPortfolios\u001b[0m:\u001b[36m362\u001b[0m - \u001b[1mStarting parallelized calculation of sorted portfolios: Univariate\u001b[0m\n",
      "\u001b[32m2025-06-22 20:20:34.721\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuildSortedPortfolios\u001b[0m:\u001b[36m363\u001b[0m - \u001b[1mSorting variable 1: bakshi_mu_tau_30, sorting variable 2: bakshiSkew_tau_30, outcome variable: return_month_ahead_excess, percentiles variable 1: [30, 70], percentiles variable 2: [30, 70]\u001b[0m\n",
      "\u001b[32m2025-06-22 20:20:34.726\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuildSortedPortfolios\u001b[0m:\u001b[36m364\u001b[0m - \u001b[1mWeighting: MarketCapWeighted, filter for portfolios: None, maximum possible backtracking: 0 days\u001b[0m\n",
      "\u001b[32m2025-06-22 20:20:34.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuildSortedPortfolios\u001b[0m:\u001b[36m376\u001b[0m - \u001b[1mChunk size: 5 meaning dates per chunk\u001b[0m\n",
      "\u001b[32m2025-06-22 20:20:34.738\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuildSortedPortfolios\u001b[0m:\u001b[36m377\u001b[0m - \u001b[1mTotal number of chunks: 44 = processes\u001b[0m\n",
      "\u001b[32m2025-06-22 20:20:34.743\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuildSortedPortfolios\u001b[0m:\u001b[36m378\u001b[0m - \u001b[1mSpawning child processes...\u001b[0m\n",
      "\u001b[32m2025-06-22 20:20:35.550\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuildSortedPortfolios\u001b[0m:\u001b[36m486\u001b[0m - \u001b[1mSaved sortedPortfolioResults to /mnt/beegfs/phd_projects/aw/courseMaterialPortfolioSorts/solution/sortedPortfolioResults.pkl\u001b[0m\n",
      "\u001b[32m2025-06-22 20:20:35.561\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuildSortedPortfolios\u001b[0m:\u001b[36m486\u001b[0m - \u001b[1mSaved sortedOutcomeResults to /mnt/beegfs/phd_projects/aw/courseMaterialPortfolioSorts/solution/sortedOutcomeResults.pkl\u001b[0m\n",
      "\u001b[32m2025-06-22 20:20:35.571\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuildSortedPortfolios\u001b[0m:\u001b[36m486\u001b[0m - \u001b[1mSaved numberOfStocks to /mnt/beegfs/phd_projects/aw/courseMaterialPortfolioSorts/solution/numberOfStocks.pkl\u001b[0m\n",
      "\u001b[32m2025-06-22 20:20:35.581\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuildSortedPortfolios\u001b[0m:\u001b[36m486\u001b[0m - \u001b[1mSaved breakpointsDict to /mnt/beegfs/phd_projects/aw/courseMaterialPortfolioSorts/solution/breakpointsDict.pkl\u001b[0m\n",
      "\u001b[32m2025-06-22 20:20:35.590\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuildSortedPortfolios\u001b[0m:\u001b[36m486\u001b[0m - \u001b[1mSaved missingDataDict to /mnt/beegfs/phd_projects/aw/courseMaterialPortfolioSorts/solution/missingDataDict.pkl\u001b[0m\n",
      "\u001b[32m2025-06-22 20:20:35.599\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuildSortedPortfolios\u001b[0m:\u001b[36m486\u001b[0m - \u001b[1mSaved missingRebalancingDatesList to /mnt/beegfs/phd_projects/aw/courseMaterialPortfolioSorts/solution/missingRebalancingDatesList.pkl\u001b[0m\n",
      "\u001b[32m2025-06-22 20:20:35.604\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mbuildSortedPortfolios\u001b[0m:\u001b[36m487\u001b[0m - \u001b[1mCompleted calculation of sorted portfolios.\u001b[0m\n",
      "\u001b[32m2025-06-22 20:20:35.614\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrunPipeline\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1mPortfolio sorts pipeline done!\u001b[0m\n",
      "\u001b[32m2025-06-22 20:20:35.620\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrunPipeline\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mEnd Time: 2025-06-22 20:20:35\u001b[0m\n",
      "\u001b[32m2025-06-22 20:20:35.626\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrunPipeline\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mDuration: 00:00:01\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from loguru import logger\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "# Directories directly relevant to this script\n",
    "baseFolder = '/mnt/beegfs/phd_projects/aw/courseMaterialPortfolioSorts' # Project folder, relevant data is stored here\n",
    "logFilePath = baseFolder + '/logfilePortfolioSortsPipeline.log' # The log file goes here\n",
    "resultsPath = baseFolder + '/solution' # Solutions go here\n",
    "\n",
    "# Parameters for parallelization (warning: check RAM load on your machine)\n",
    "num_processes = 50 # Number of processes that are run in parallel\n",
    "chunk_size = 6 # number of objects that each process works on\n",
    "\n",
    "# Other important parameters\n",
    "startDate = pd.to_datetime('01.01.2005', format='%d.%m.%Y') # start date for the procedure\n",
    "endDate = pd.to_datetime('01.01.2023', format='%d.%m.%Y') # end date for the procedure\n",
    "sortingType = \"Univariate\" # alternatively \"BivariateDependent\" or \"BivariateIndependent\"\n",
    "marketCapColumn = \"MCap\" # name of the column that contains the market capitalization\n",
    "sortVariable1 = \"bakshi_mu_tau_30\" # name of the first sorting variable\n",
    "sortVariable2 = \"bakshiSkew_tau_30\" # name of the second sorting variable\n",
    "outcomeVariable = \"return_month_ahead_excess\" # name of the outcome variable\n",
    "sortPercentilesVar1 = [30, 70] # percentiles for the split of sort variable 1\n",
    "sortPercentilesVar2 = [30, 70] # percentiles for the split of sort variable 2\n",
    "weightType = \"MarketCapWeighted\" # weighting scheme within the portfolios, alternatively \"EquallyWeighted\"\n",
    "filterPortfolios = None # Optional parameter for filtering out sorted portfolios by index\n",
    "backtrackingDays = 0 # Variable to handle the number of days of backtracking for portfolio sorts, values other than 0 are not supported in this code version\n",
    "lags = 6 # the number of lags for the Newey-West adjustment for the statistical tests\n",
    "testmode = False # if test mode is set to true, the sorting of portfolios will only be applied to a small number of dates\n",
    "log_results = False # Variable to control whether the results of the portfolio sorting are printed in the log\n",
    "\n",
    "# define class that handles all computations and logic\n",
    "class PortfolioSortsPipeline():\n",
    "\n",
    "    def runPipeline(self):\n",
    "        # This function is where all steps of the procedure are called one after the other\n",
    "        # Set up logging\n",
    "        start_time = time.time()\n",
    "        if os.path.exists(logFilePath):\n",
    "            os.remove(logFilePath)\n",
    "        logger.add(logFilePath, level='INFO')\n",
    "        logger.info(f\"Start Time: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(start_time))}\")\n",
    "        logger.info('Start portfolio sorts pipeline...')\n",
    "        #-----------------\n",
    "        # Each function performs a step in the procedure\n",
    "        #self.calcMonthlyExcessReturn()\n",
    "        #self.mergeData()\n",
    "        #self.getSortingDates()\n",
    "        #self.buildSortedPortfolios()\n",
    "        #self.convertOutcome()\n",
    "        #self.calcDiffPortfolios()\n",
    "        #self.performStatTest()\n",
    "        #-----------------\n",
    "        end_time = time.time()\n",
    "        duration_seconds = end_time - start_time\n",
    "        duration_formatted = time.strftime('%H:%M:%S', time.gmtime(duration_seconds))\n",
    "        logger.info('Portfolio sorts pipeline done!')\n",
    "        logger.info(f\"End Time: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(end_time))}\")\n",
    "        logger.info(f'Duration: {duration_formatted}')\n",
    "\n",
    "    def calcMonthlyExcessReturn(self):\n",
    "        # File Paths\n",
    "        prices_filepath = baseFolder + '/SPXConstituentsPrices.parquet'\n",
    "        riskfree_filepath = baseFolder + '/RiskFreeRatesFiltered.parquet'\n",
    "        riskfree_output = resultsPath + \"/RiskFreeRatesFilteredMonthly.parquet\"\n",
    "        prices_output = resultsPath + \"/SPXConstituentsPricesPrepared.parquet\"\n",
    "\n",
    "        # Load Prices and Risk-Free Rates\n",
    "        pricesData = pd.read_parquet(prices_filepath)\n",
    "        riskFreeRatesData = pd.read_parquet(riskfree_filepath)\n",
    "\n",
    "        # Convert 'date' columns to pd.Timestamp for both dataframes\n",
    "        pricesData['date'] = pd.to_datetime(pricesData['date'])\n",
    "        riskFreeRatesData['date'] = pd.to_datetime(riskFreeRatesData['date'])\n",
    "\n",
    "        ### A - Risk Free Rates ###\n",
    "        # Filter for daystomaturity = 30 and drop unnecessary columns\n",
    "        ########## STUDENT TASK START ##########\n",
    "        riskFreeRatesData = riskFreeRatesData[riskFreeRatesData['daystomaturity'] == 30]\n",
    "        ########## STUDENT TASK END ##########\n",
    "        riskFreeRatesData = riskFreeRatesData.drop(columns=['yld_pct_annual', 'yld_pct_daily'])\n",
    "\n",
    "        # Save the filtered risk-free rates\n",
    "        riskFreeRatesData.to_parquet(riskfree_output, index=False)\n",
    "        logger.info(f\"Processed risk free rates saved to {riskfree_output}\")\n",
    "\n",
    "        ### B - Prices Data Processing ###\n",
    "        # Drop unnecessary columns\n",
    "        cols_to_drop = ['bidlow', 'askhigh', 'volume', 'totalreturn', 'adjustmentfactor', 'openprice', 'sharesoutstanding']\n",
    "        ########## STUDENT TASK START ##########\n",
    "        pricesData = pricesData.drop(columns=cols_to_drop)\n",
    "        ########## STUDENT TASK END ##########\n",
    "\n",
    "        # Filter by date range\n",
    "        pricesData = pricesData[(pricesData['date'] >= startDate) & (pricesData['date'] <= endDate)]\n",
    "\n",
    "        # Adjusted Close Price Calculation\n",
    "        pricesData['adjustedclose'] = (pricesData['closeprice'] * pricesData['adjustmentfactor2']) / pricesData.groupby('securityid')['adjustmentfactor2'].transform('last')\n",
    "\n",
    "        # Monthly Discrete Returns (20-trading-day rolling return)\n",
    "        pricesData['return_month'] = pricesData.groupby('securityid')['adjustedclose'].pct_change(periods=20)\n",
    "\n",
    "        # Merge with Risk-Free Rates on 'date'\n",
    "        pricesData = pricesData.merge(riskFreeRatesData, on='date', how='left')\n",
    "\n",
    "        # Calculate Monthly Excess Return\n",
    "        pricesData['return_month_excess'] = pricesData['return_month'] - pricesData['yld_pct_monthly']\n",
    "\n",
    "        # Offset Monthly Excess Return by 20 trading days into the past\n",
    "        ########## STUDENT TASK START ##########\n",
    "        pricesData['return_month_ahead_excess'] = pricesData.groupby('securityid')['return_month_excess'].shift(-20)\n",
    "        ########## STUDENT TASK END ##########\n",
    "\n",
    "        # Drop some columns\n",
    "        pricesData = pricesData.drop(columns=['closeprice', 'adjustmentfactor2', 'daystomaturity', 'yld_pct_monthly'])\n",
    "\n",
    "        # Save the final processed dataset\n",
    "        pricesData.to_parquet(prices_output, index=False)\n",
    "        logger.info(f\"Processed data saved to {prices_output}\")\n",
    "\n",
    "    def mergeData(self):\n",
    "        # File paths for the data\n",
    "        prices_filepath = resultsPath + \"/SPXConstituentsPricesPrepared.parquet\"\n",
    "        cram_filepath = baseFolder + \"/CRAMMeasures.parquet\"\n",
    "        merged_output = resultsPath + \"/MergedInputData.parquet\"\n",
    "\n",
    "        # Load the data\n",
    "        pricesData = pd.read_parquet(prices_filepath)\n",
    "        cramData = pd.read_parquet(cram_filepath)\n",
    "\n",
    "        # Ensure 'date' in pricesData and 'loctimestamp' in cramData are in the same format\n",
    "        ########## STUDENT TASK START ##########\n",
    "        pricesData['date'] = pd.to_datetime(pricesData['date'])\n",
    "        cramData['loctimestamp'] = pd.to_datetime(cramData['loctimestamp'])\n",
    "        ########## STUDENT TASK END ##########\n",
    "\n",
    "        # Filter CRAM data by date\n",
    "        cramData = cramData[(cramData['loctimestamp'] >= startDate) & (cramData['loctimestamp'] <= endDate)]\n",
    "\n",
    "        # Merge the data on SecurityID and date/loctimestamp\n",
    "        mergedData = pd.merge(\n",
    "            cramData,\n",
    "            pricesData[['securityid', 'date', 'return_month_ahead_excess']],\n",
    "            how='left',\n",
    "            left_on=['SecurityID', 'loctimestamp'],\n",
    "            right_on=['securityid', 'date']\n",
    "        )\n",
    "\n",
    "        # Drop the extra 'date' column that was added from pricesData\n",
    "        ########## STUDENT TASK START ##########\n",
    "        mergedData = mergedData.drop(columns=['date'])\n",
    "        ########## STUDENT TASK END ##########\n",
    "\n",
    "        # Save the merged data to a new Parquet file\n",
    "        mergedData.to_parquet(merged_output, index=False)\n",
    "\n",
    "        logger.info(f\"Merged data saved to {merged_output}\")\n",
    "\n",
    "    def getSortingDates(self):\n",
    "\n",
    "        # File paths\n",
    "        merged_filepath = resultsPath + \"/MergedInputData.parquet\"\n",
    "        filtered_output = resultsPath + \"/MergedInputDataFiltered.parquet\"\n",
    "        sorting_dates_output = resultsPath + \"/SortingDates.parquet\"\n",
    "\n",
    "        # Load merged data\n",
    "        mergedData = pd.read_parquet(merged_filepath)\n",
    "\n",
    "        # Ensure 'loctimestamp' is a datetime object\n",
    "        mergedData['loctimestamp'] = pd.to_datetime(mergedData['loctimestamp'])\n",
    "\n",
    "        # Extract unique dates\n",
    "        unique_dates = sorted(mergedData['loctimestamp'].unique())  # Sort in ascending order\n",
    "\n",
    "        # Create a dataframe from the unique dates\n",
    "        unique_dates_df = pd.DataFrame({'loctimestamp': unique_dates})\n",
    "\n",
    "        # Find the first available date for each month\n",
    "        unique_dates_df['year_month'] = unique_dates_df['loctimestamp'].dt.to_period('M')  # Extract year-month\n",
    "        first_of_month_dates = unique_dates_df.groupby('year_month')['loctimestamp'].min().reset_index(drop=True)\n",
    "\n",
    "        # Filter original dataframe to keep only rows with these first-of-month dates\n",
    "        ########## STUDENT TASK START ##########\n",
    "        filteredData = mergedData[mergedData['loctimestamp'].isin(first_of_month_dates)]\n",
    "        ########## STUDENT TASK END ##########\n",
    "\n",
    "        # Reset index before saving\n",
    "        filteredData = filteredData.reset_index(drop=True)\n",
    "        first_of_month_dates = first_of_month_dates.reset_index(drop=True)\n",
    "\n",
    "        # Save the filtered dataframe\n",
    "        filteredData.to_parquet(filtered_output, index=False)\n",
    "\n",
    "        # Save the list of first-of-month dates\n",
    "        first_of_month_dates.to_frame(name=\"first_of_month\").to_parquet(sorting_dates_output, index=False)\n",
    "\n",
    "        logger.info(f\"Filtered dataset saved to {filtered_output}\")\n",
    "        logger.info(f\"Sorting dates saved to {sorting_dates_output}\")\n",
    "\n",
    "    def buildSortedPortfoliosChild(self, instrumentData, dateChunk, entitiesByDates, sortingType, marketCapColumn, sortVariable1, sortVariable2, outcomeVariable, sortPercentilesVar1, sortPercentilesVar2, weightType, filterPortfolios, backtrackingDays, resultsQueue, missingDataQueue):\n",
    "                # Process a subset of dates and return data of sorted portfolios and encountered errors to the result queues\n",
    "        pid = os.getpid() # Variable to store the process ID, to be able to identify the child process\n",
    "        #logger.info(f'Process {pid} started with {len(dateChunk)} dates.')\n",
    "        portfolioWeightsDictChild = {} # Key: portfolioID, Value: {date: weights}\n",
    "        outcomeResultsDictChild = {} # Key: portfolioID, Value: {date: averageOutcome}\n",
    "        breakpointsDictChild = {} # breakpoints per date\n",
    "        missingDataDictChild = {} # Key: date, Value: {errortype: count}\n",
    "        missingRebalancingDatesListChild = [] # Dates for which no portfolio weights are calculated due to missing data\n",
    "        allInstruments = instrumentData['instrumentid'].unique().tolist()\n",
    "        # Function to handle possible duplicate column names when counting errors\n",
    "        def countNaNs(df, colname):\n",
    "            col = df[colname]\n",
    "            if isinstance(col, pd.DataFrame): # If column name, i.e. variable, comes up more than once\n",
    "                return col.iloc[:, 0].isna().sum()\n",
    "            return col.isna().sum()\n",
    "        # Iterate over each date and perform sorting\n",
    "        for date in dateChunk:\n",
    "            workingDate = date\n",
    "            validDataFound = False\n",
    "            backtrackAttempts = 0\n",
    "            missingData = {date: {\"instruments_missing\": 0, \"sortVariable1_nan\": 0, \"sortVariable2_nan\": 0, \"marketCap_nan\": 0, \"outcomeVar_nan\": 0, \"portfolio_insufficient\": 0}}\n",
    "            if entitiesByDates is not None and not entitiesByDates.empty:\n",
    "                constituents = entitiesByDates[entitiesByDates['Date'] == date]['InstrumentID']\n",
    "                ####################################################################################################################################################################################\n",
    "                # Duplicate instrumentIDs should never occur!\n",
    "                # Fix this somewhere else, i.e in the mapping, then remove this code block!\n",
    "                # Look at this InsID: USOPT0003356D1\n",
    "                duplicateIDs = constituents[constituents.duplicated(keep=False)] # Check for duplicates in InstrumentIDs\n",
    "                if not duplicateIDs.empty:\n",
    "                    logger.error(f'Process {pid}: Dropping duplicate InstrumentIDs found on {date}: {duplicateIDs.unique()}')\n",
    "                constituents = constituents.drop_duplicates()\n",
    "                ####################################################################################################################################################################################\n",
    "            else:\n",
    "                constituents = allInstruments\n",
    "            if sortingType == \"Univariate\":\n",
    "                while not validDataFound and backtrackAttempts <= backtrackingDays:\n",
    "                    # Try to find matching data\n",
    "                    dateDataframe = instrumentData[ # filter the dataframe in a vectorized manner\n",
    "                        (instrumentData['loctimestamp'] == workingDate) &\n",
    "                        (instrumentData['instrumentid'].isin(constituents))\n",
    "                    ][['instrumentid', sortVariable1, marketCapColumn, outcomeVariable]]\n",
    "                    validDataframe = dateDataframe.dropna(subset=[sortVariable1, marketCapColumn, outcomeVariable])\n",
    "                    instrumentDataValid = list(validDataframe.itertuples(index=False, name=None)) # Get instrument data\n",
    "                    # Step 1: Determine values of the sort variable\n",
    "                    ########## STUDENT TASK START ##########\n",
    "                    sort1Values = [x[1] for x in instrumentDataValid]\n",
    "                    ########## STUDENT TASK END ##########\n",
    "                    if len(sort1Values) > 0:\n",
    "                        validDataFound = True\n",
    "                    else:\n",
    "                        logger.warning(f\"{pid} No valid CRAM data found for {workingDate}. Backtracking to the previous date.\")\n",
    "                        backtrackAttempts += 1\n",
    "                        workingDate = workingDate - pd.Timedelta(days=1)\n",
    "                if backtrackAttempts > backtrackingDays:\n",
    "                    logger.error(f\"{pid} ERROR Failed to find valid CRAM data for {date} after {backtrackingDays} attempts.\")\n",
    "                    missingRebalancingDatesListChild.append(date)\n",
    "                    continue # Stop searching and log error for the original date.\n",
    "                # Count instruments with no data\n",
    "                missingIDs = set(constituents) - set(dateDataframe['instrumentid'])\n",
    "                missingData[date][\"instruments_missing\"] += len(missingIDs)\n",
    "                # Count NaNs\n",
    "                missingData[date][\"sortVariable1_nan\"] += int(countNaNs(dateDataframe, sortVariable1))\n",
    "                missingData[date][\"marketCap_nan\"]     += int(countNaNs(dateDataframe, marketCapColumn))\n",
    "                missingData[date][\"outcomeVar_nan\"]    += int(countNaNs(dateDataframe, outcomeVariable))\n",
    "                missingDataDictChild[date] = missingData[date]\n",
    "                ########## STUDENT TASK START ##########\n",
    "                breakpoints = np.percentile(sort1Values, sortPercentilesVar1) # Step 2: Calculate breakpoints\n",
    "                breakpoints = [float(\"-inf\")] + list(breakpoints) + [float(\"inf\")]\n",
    "                ########## STUDENT TASK END ##########\n",
    "                portfolios = {i + 1: [] for i in range(len(breakpoints) - 1)} # Initialize dictionary structure to put portfolios in\n",
    "                for instrumentID, sort1Value, marketCap, outcomeValue in instrumentDataValid: # Step 3: Sort instruments based on breakpoints\n",
    "                    ########## STUDENT TASK START ##########\n",
    "                    for i in range(len(breakpoints) - 1):\n",
    "                        if breakpoints[i] <= sort1Value <= breakpoints[i + 1]:\n",
    "                            portfolios[i + 1].append((instrumentID, sort1Value, marketCap, outcomeValue))\n",
    "                            # Notice: No break of the loop, the instrument can be in multiple portfolios\n",
    "                    ########## STUDENT TASK END ##########\n",
    "\n",
    "                for portfolioID, instruments in portfolios.items(): # remove duplicate instruments for each portfolio\n",
    "                    portfolios[portfolioID] = list({x[0]: x for x in instruments}.values())\n",
    "\n",
    "                # Check for duplicates in each portfolio\n",
    "                for portfolioID, instruments in portfolios.items():\n",
    "                    instrumentIDs = [x[0] for x in instruments]  # Extract instrument IDs\n",
    "                    if len(set(instrumentIDs)) != len(instrumentIDs):\n",
    "                        duplicateIDs = [x for x in instrumentIDs if instrumentIDs.count(x) > 1]\n",
    "                        logger.error(f\"Duplicates found in portfolio {portfolioID} on date {date}: {set(duplicateIDs)}\")\n",
    "                        assert False, f\"Duplicates in portfolio {portfolioID} on date {date}\"\n",
    "\n",
    "                # Step 4: Filter portfolios (optional)\n",
    "                if filterPortfolios:\n",
    "                    filteredPortfolios = {f\"{sortVariable1}_\" + str(i): portfolios[i] for i in filterPortfolios if i in portfolios}\n",
    "                else:\n",
    "                    filteredPortfolios = {f\"{sortVariable1}_\" + str(i): portfolios[i] for i in portfolios}\n",
    "\n",
    "                # Step 5: Calculate Weights & average outcome variable\n",
    "                portfolioWeights = {}\n",
    "                portfolioOutcome = {}\n",
    "                for portfolioID, instruments in filteredPortfolios.items():\n",
    "                    ########## STUDENT TASK END ##########\n",
    "                    if weightType == \"EquallyWeighted\":\n",
    "                        weight = 1 / len(instruments)\n",
    "                        portfolioWeights[portfolioID] = {instrumentID: weight for instrumentID, _, _, _ in instruments} # weights for each instrument in the portfolio\n",
    "                        weightedSum = sum(weight * outcomeValue for _, _, _, outcomeValue in instruments) # weighted outcome variable for each portfolio\n",
    "                    elif weightType == \"MarketCapWeighted\":\n",
    "                        totalMarketCap = sum(marketCap for _, _, marketCap, _ in instruments)\n",
    "                        portfolioWeights[portfolioID] = {instrumentID: marketCap / totalMarketCap for instrumentID, _, marketCap, _ in instruments}\n",
    "                        weightedSum = sum((marketCap / totalMarketCap) * outcomeValue for _, _, marketCap, outcomeValue in instruments)\n",
    "                    portfolioOutcome[portfolioID] = weightedSum\n",
    "                    ########## STUDENT TASK END ##########\n",
    "\n",
    "                # Collect results\n",
    "                for portfolioID, weightStructure in portfolioWeights.items():\n",
    "                    portfolioWeightsAtDate = {date: weightStructure}\n",
    "                    if portfolioID not in portfolioWeightsDictChild:\n",
    "                        portfolioWeightsDictChild[portfolioID] = {} # Initialize portfolioID entry\n",
    "                    for date, weights in portfolioWeightsAtDate.items():\n",
    "                        portfolioWeightsDictChild[portfolioID][date] = weights # Update with new data\n",
    "\n",
    "                for portfolioID, outcome in portfolioOutcome.items():\n",
    "                    portfolioOutcomeAtDate = {date: outcome}\n",
    "                    if portfolioID not in outcomeResultsDictChild:\n",
    "                        outcomeResultsDictChild[portfolioID] = {} # Initialize portfolioID entry\n",
    "                    for date, outcome in portfolioOutcomeAtDate.items():\n",
    "                        outcomeResultsDictChild[portfolioID][date] = outcome\n",
    "\n",
    "                # Set up breakpoint dictionary\n",
    "                if date not in breakpointsDictChild.keys():\n",
    "                    breakpointsDictChild[date] = {}\n",
    "                breakpointsDictChild[date][sortVariable1] = breakpoints\n",
    "\n",
    "            elif sortingType in [\"BivariateDependent\", \"BivariateIndependent\"]:\n",
    "                ########## STUDENT TASK LEVEL 2 START ##########\n",
    "                # Implement the logic for bivariate sorting following the logic of the univariate sort\n",
    "                # You can do dependent or independent sorting (or optionally even both, if you like)\n",
    "                # One can copy the code for the univariate sort and adjust the relevant parts here\n",
    "                ########## STUDENT TASK LEVEL 2 END ##########\n",
    "                logger.info(f\"Bivariate sorting is not implemented!\")\n",
    "                break\n",
    "\n",
    "        # Pass results to queue\n",
    "        resultsQueue.put((portfolioWeightsDictChild, breakpointsDictChild, outcomeResultsDictChild))\n",
    "        missingDataQueue.put((missingDataDictChild, missingRebalancingDatesListChild))\n",
    "        #logger.info(f'Process {pid} done.')\n",
    "\n",
    "    def buildSortedPortfolios(self):\n",
    "        # File paths\n",
    "        instrument_data = resultsPath + \"/MergedInputDataFiltered.parquet\"\n",
    "        instrumentData = pd.read_parquet(instrument_data)\n",
    "        sorting_dates = resultsPath + \"/SortingDates.parquet\"\n",
    "        sortingDates = pd.read_parquet(sorting_dates)\n",
    "        sortingDates = sortingDates[\"first_of_month\"].tolist() # convert to list\n",
    "        if testmode:\n",
    "            sortingDates = sortingDates[0:5] # Short list for testing\n",
    "        entitiesByDates = None # here one could provide a dataframe to determine date specific constituents for algorithm, not relevant in this code version\n",
    "        logger.info(f'Starting parallelized calculation of sorted portfolios: {sortingType}')\n",
    "        logger.info(f'Sorting variable 1: {sortVariable1}, sorting variable 2: {sortVariable2}, outcome variable: {outcomeVariable}, percentiles variable 1: {sortPercentilesVar1}, percentiles variable 2: {sortPercentilesVar2}')\n",
    "        logger.info(f'Weighting: {weightType}, filter for portfolios: {filterPortfolios}, maximum possible backtracking: {backtrackingDays} days')\n",
    "        portfolioWeightsDict = {} # Key: portfolioID, Value: {date: weights}\n",
    "        outcomeResultsDict = {} # Key: portfolioID, Value: {date: averageOutcome}\n",
    "        numberOfStocksDict = {} # Number of stocks per portfolio\n",
    "        breakpointsDict = {} # Breakpoints per date\n",
    "        missingDataDict = {} # Key: date, Value: {errortype: count}\n",
    "        missingRebalancingDatesList = [] # List containing missing rebalancing dates\n",
    "        chunkSize = len(sortingDates) // num_processes + 1 # Divide dates into chunks and handle each chunk in a separate process\n",
    "        dateChunks = [sortingDates[i:i + chunkSize] for i in range(0, len(sortingDates), chunkSize)] # List containing lists (chunks) of dates\n",
    "        resultsQueue = mp.Queue() # Process queue which will collect results of portfolio sorts\n",
    "        missingDataQueue = mp.Queue() # Second process queue which will collect missing data information\n",
    "        processes = [] # Empty list to contain child processes\n",
    "        logger.info(f'Chunk size: {chunkSize} meaning dates per chunk')\n",
    "        logger.info(f'Total number of chunks: {len(dateChunks)} = processes')\n",
    "        logger.info(f'Spawning child processes...')\n",
    "        for dateChunk in dateChunks: # Each chunk gets passed to one child process\n",
    "            p = mp.Process(\n",
    "                target=self.buildSortedPortfoliosChild,\n",
    "                args=( # The child process gets passed all relevant parameters\n",
    "                    instrumentData, dateChunk, entitiesByDates,\n",
    "                    sortingType, marketCapColumn, sortVariable1, sortVariable2, outcomeVariable,\n",
    "                    sortPercentilesVar1, sortPercentilesVar2,\n",
    "                    weightType, filterPortfolios, backtrackingDays,\n",
    "                    resultsQueue, missingDataQueue))\n",
    "            processes.append(p)\n",
    "            p.start()\n",
    "        # Fetch results from queues\n",
    "        for _ in range(len(dateChunks)): # Process results for each chunk\n",
    "            portfolioWeightsDictChild, breakpointsDictChild, outcomeResultsDictChild = resultsQueue.get()\n",
    "            missingDataDictChild, missingRebalancingDatesListChild = missingDataQueue.get()\n",
    "            # Collect portfolio weights\n",
    "            for portfolioID, weightDict in portfolioWeightsDictChild.items():\n",
    "                if portfolioID not in portfolioWeightsDict:\n",
    "                    portfolioWeightsDict[portfolioID] = {} # Initialize portfolioID entry\n",
    "                for date, weights in weightDict.items():\n",
    "                    portfolioWeightsDict[portfolioID][date] = weights # Update with new data\n",
    "            # Collect average outcome variables\n",
    "            for portfolioID, outcomeDict in outcomeResultsDictChild.items():\n",
    "                if portfolioID not in outcomeResultsDict:\n",
    "                    outcomeResultsDict[portfolioID] = {} # Initialize\n",
    "                for date, outcome in outcomeDict.items():\n",
    "                    outcomeResultsDict[portfolioID][date] = outcome # Fill data in\n",
    "            # Collect missing data stats\n",
    "            for date, dataInfo in missingDataDictChild.items():\n",
    "                missingDataDict[date] = dataInfo\n",
    "            # Collect breakpoints\n",
    "            for date, dataDict in breakpointsDictChild.items():\n",
    "                if date not in breakpointsDict:\n",
    "                    breakpointsDict[date] = {} # Initialize portfolioID entry\n",
    "                for variable, breakpoints in dataDict.items():\n",
    "                    if variable not in breakpointsDict[date]:\n",
    "                        breakpointsDict[date][variable] = {} # Initialize date entry\n",
    "                    breakpointsDict[date][variable] = breakpoints # Update with new data\n",
    "            if missingRebalancingDatesListChild:\n",
    "                missingRebalancingDatesList.append(missingRebalancingDatesListChild)\n",
    "\n",
    "        for p in processes: # Wait for all processes to finish\n",
    "            p.join()\n",
    "\n",
    "        # Sort portfolio results by date ascending for each portfolio\n",
    "        sortedPortfolioWeightsDict = {\n",
    "            portfolioID: dict(sorted(weightsByDate.items()))\n",
    "            for portfolioID, weightsByDate in portfolioWeightsDict.items()\n",
    "            }\n",
    "        # Sort outcome results by date ascending for each portfolio\n",
    "        sortedOutcomeResultsDict = {\n",
    "            portfolioID: dict(sorted(outcomeByDate.items()))\n",
    "            for portfolioID, outcomeByDate in outcomeResultsDict.items()\n",
    "            }\n",
    "        # Sort missingData by date ascending\n",
    "        missingDataDict = dict(sorted(missingDataDict.items(), key=lambda item: item[0]))\n",
    "        # Sort breakpoints by date ascending\n",
    "        breakpointsDict = dict(sorted(breakpointsDict.items(), key=lambda item: item[0]))\n",
    "        # Count number of stocks\n",
    "        for portfolioID, portfolioWeights in sortedPortfolioWeightsDict.items():\n",
    "            if portfolioID not in numberOfStocksDict:\n",
    "                numberOfStocksDict[portfolioID] = {}\n",
    "            for date, weights in portfolioWeights.items():\n",
    "                numberOfStocksDict[portfolioID][date] = len(weights)\n",
    "        ##################### PRINT RESULTS TO LOG ########################\n",
    "        if log_results:\n",
    "            logger.info(\"------Portfolios:-------\")\n",
    "            for portfolio, weightStructure in sortedPortfolioWeightsDict.items():\n",
    "                logger.info(f'Portfolio {portfolio}:')\n",
    "                for date, weights in weightStructure.items():\n",
    "                    logger.info(f'Date {date}: Something')\n",
    "            logger.info(\"------Outcome:-------\")\n",
    "            for portfolio, outcomeStruct in sortedOutcomeResultsDict.items():\n",
    "                logger.info(f'Portfolio {portfolio}:')\n",
    "                for date, outcome in outcomeStruct.items():\n",
    "                    logger.info(f'Date {date}: {outcome}')\n",
    "            logger.info(\"------Missing Data:------\")\n",
    "            for date, dataInfo in missingDataDict.items():\n",
    "                logger.info(date)\n",
    "                logger.info(dataInfo)\n",
    "            logger.info(\"------Missing rebalancing dates:------\")\n",
    "            logger.info(missingRebalancingDatesList)\n",
    "            logger.info(\"------Number of stocks:------\")\n",
    "            for portfolio, nstocksStruct in numberOfStocksDict.items():\n",
    "                logger.info(f'Portfolio {portfolio}:')\n",
    "                for date, nstocks in nstocksStruct.items():\n",
    "                    logger.info(f'Date {date}: {nstocks}')\n",
    "            logger.info(\"------Breakpoints:------\")\n",
    "            for date, struct in breakpointsDict.items():\n",
    "                logger.info(f'date: {date}')\n",
    "                for var, bps in struct.items():\n",
    "                    logger.info(f'variable: {var}')\n",
    "                    logger.info(f'breakpoints: {bps}')\n",
    "        # Dictionary of variables to save\n",
    "        variables_to_save = {\n",
    "            \"sortedPortfolioResults\": sortedPortfolioWeightsDict,\n",
    "            \"sortedOutcomeResults\": sortedOutcomeResultsDict,\n",
    "            \"numberOfStocks\": numberOfStocksDict,\n",
    "            \"breakpointsDict\": breakpointsDict,\n",
    "            \"missingDataDict\": missingDataDict,\n",
    "            \"missingRebalancingDatesList\": missingRebalancingDatesList\n",
    "        }\n",
    "        # Loop through dictionary and save each variable as a .pkl file\n",
    "        for var_name, var_value in variables_to_save.items():\n",
    "            file_path = os.path.join(resultsPath, f\"{var_name}.pkl\")\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                pickle.dump(var_value, f)\n",
    "            logger.info(f\"Saved {var_name} to {file_path}\")\n",
    "        logger.info('Completed calculation of sorted portfolios.')\n",
    "\n",
    "    def convertOutcome(self):\n",
    "        # Path to the input pickle file\n",
    "        filtered_output = resultsPath + \"/sortedOutcomeResults.pkl\"\n",
    "        \n",
    "        # Load the sortedOutcomeResults from pickle\n",
    "        with open(filtered_output, 'rb') as f:\n",
    "            sortedOutcomeResults = pickle.load(f)\n",
    "        \n",
    "        # Create a list to store all the dates (we assume the dates are the same for all portfolios)\n",
    "        ########## STUDENT TASK START ##########\n",
    "        dates = list(next(iter(sortedOutcomeResults.values())).keys())\n",
    "        ########## STUDENT TASK END ##########\n",
    "\n",
    "        # Create a dictionary where keys are portfolio names and values are the corresponding outcome values for each date\n",
    "        portfolio_data = {}\n",
    "        \n",
    "        for portfolio, outcomeStruct in sortedOutcomeResults.items():\n",
    "            # Create a list of outcomes for the given portfolio, aligned with the dates\n",
    "            portfolio_data[portfolio] = [outcomeStruct.get(date, None) for date in dates]\n",
    "\n",
    "        # Convert the dictionary into a DataFrame\n",
    "        outcome_df = pd.DataFrame(portfolio_data, index=dates)\n",
    "        \n",
    "        # Reset index so 'date' becomes a column in the DataFrame\n",
    "        outcome_df.reset_index(inplace=True)\n",
    "        outcome_df.rename(columns={'index': 'date'}, inplace=True)\n",
    "\n",
    "        # Save the DataFrame to a Parquet file\n",
    "        outcome_df.to_parquet(resultsPath + \"/PortfoliosOutcome.parquet\", index=False)\n",
    "        logger.info(\"PortfoliosOutcome.parquet has been saved.\")\n",
    "\n",
    "    def calcDiffPortfolios(self):\n",
    "        # Load the PortfoliosOutcome.parquet file\n",
    "        portfolios_outcome_filepath = resultsPath + \"/PortfoliosOutcome.parquet\"\n",
    "        outcome_df = pd.read_parquet(portfolios_outcome_filepath)\n",
    "\n",
    "        if sortingType == \"Univariate\":\n",
    "\n",
    "            # Determine the portfolio names for the highest and lowest portfolios based on sortVariable1\n",
    "            high_portfolio = f\"{sortVariable1}_{len(sortPercentilesVar1) + 1}\"  # Highest portfolio\n",
    "            low_portfolio = f\"{sortVariable1}_1\"  # Lowest portfolio\n",
    "            \n",
    "            # Check if the high and low portfolios exist in the columns\n",
    "            ########## STUDENT TASK START ##########\n",
    "            if high_portfolio not in outcome_df.columns or low_portfolio not in outcome_df.columns:\n",
    "                logger.error(f\"Portfolio columns {high_portfolio} or {low_portfolio} not found in the DataFrame.\")\n",
    "                return\n",
    "            ########## STUDENT TASK END ##########\n",
    "            \n",
    "            # Add the new column: sortVariable1_Diff (difference between high and low portfolio)\n",
    "            ########## STUDENT TASK START ##########\n",
    "            outcome_df[f\"{sortVariable1}_Diff\"] = outcome_df[high_portfolio] - outcome_df[low_portfolio]\n",
    "            ########## STUDENT TASK END ##########\n",
    "\n",
    "            # Save the resulting DataFrame to a new Parquet file\n",
    "            output_filepath = resultsPath + \"/PortfoliosOutcomePlusDiff.parquet\"\n",
    "            outcome_df.to_parquet(output_filepath, index=False)\n",
    "            \n",
    "            logger.info(f\"Resulting DataFrame with the new 'Diff' column saved to {output_filepath}\")\n",
    "\n",
    "        elif sortingType in (\"BivariateDependent\", \"BivariateIndependent\"):\n",
    "            ########## STUDENT TASK LEVEL 2 START ##########\n",
    "            # Implement the logic for bivariate sorting following the logic of the univariate sort\n",
    "            # You can do dependent or independent sorting (or optionally even both, if you like)\n",
    "            # One can copy the code for the univariate sort and adjust the relevant parts here\n",
    "            ########## STUDENT TASK LEVEL 2 END ##########\n",
    "            logger.info(f\"Creating diff portfolios for bivariate sorted portfolios is not implemented!\")\n",
    "\n",
    "    def performStatTest(self):\n",
    "        # Load the PortfoliosOutcomePlusDiff.parquet file\n",
    "        portfolios_outcome_filepath = resultsPath + \"/PortfoliosOutcomePlusDiff.parquet\"\n",
    "        outcome_df = pd.read_parquet(portfolios_outcome_filepath)\n",
    "\n",
    "        # Exclude the 'date' column and work with portfolio columns\n",
    "        portfolio_columns = [col for col in outcome_df.columns if col != 'date']\n",
    "\n",
    "        # Initialize a list to hold the statistics (Avg, StdErr, t-stat, p-value)\n",
    "        stats_dict = {\n",
    "            \"Average\": [],\n",
    "            \"StdErr\": [],\n",
    "            \"t-stat\": [],\n",
    "            \"p-value\": []\n",
    "        }\n",
    "\n",
    "        # Calculate the statistics for each portfolio\n",
    "        for portfolio in portfolio_columns:\n",
    "            # Get the time series for the portfolio\n",
    "            timeseries = outcome_df[portfolio]\n",
    "\n",
    "            # Calculate the average (mean) of the timeseries\n",
    "            ########## STUDENT TASK START ##########\n",
    "            avg = np.mean(timeseries)\n",
    "            ########## STUDENT TASK END ##########\n",
    "            stats_dict[\"Average\"].append(avg)\n",
    "\n",
    "            # Calculate the standard error of the timeseries\n",
    "            ########## STUDENT TASK START ##########\n",
    "            std_err = np.std(timeseries, ddof=1) / np.sqrt(len(timeseries))\n",
    "            ########## STUDENT TASK END ##########\n",
    "            stats_dict[\"StdErr\"].append(std_err)\n",
    "\n",
    "            # Perform OLS regression with constant (Newey-West adjustment with 6 lags)\n",
    "            ########## STUDENT TASK START ##########\n",
    "            model = OLS(timeseries, add_constant(np.ones(len(timeseries)))) # OLS regression with constant\n",
    "            results = model.fit(cov_type='HAC', cov_kwds={'maxlags': lags})\n",
    "            ########## STUDENT TASK END ##########\n",
    "            \n",
    "            # Extract the t-statistic for the mean\n",
    "            t_stat = results.tvalues.iloc[0]  # t-value for the constant term\n",
    "            stats_dict[\"t-stat\"].append(t_stat)\n",
    "\n",
    "            # Calculate the p-value from the t-statistic\n",
    "            p_value = results.pvalues.iloc[0] # p-value for the constant term\n",
    "            stats_dict[\"p-value\"].append(p_value)\n",
    "\n",
    "        # Create a DataFrame from the statistics dictionary\n",
    "        stats_df = pd.DataFrame(stats_dict, index=portfolio_columns)\n",
    "\n",
    "        # Save the resulting DataFrame to a new Parquet file\n",
    "        output_filepath = resultsPath + \"/PortfoliosOutcomePlusDiffTest.parquet\"\n",
    "        stats_df.to_parquet(output_filepath, index=True)\n",
    "\n",
    "        logger.info(f\"Statistical test results saved to {output_filepath}\")\n",
    "\n",
    "#--------------------\n",
    "\n",
    "# Create object and run all steps\n",
    "PortfolioSortsPipeline().runPipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
